{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape:(60000, 28, 28), y shape:(60000,)\n",
      "batch: (128, 28, 28) (128,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, optimizers, Sequential, metrics\n",
    "from matplotlib import pyplot as plt\n",
    "import  io\n",
    "\n",
    "(x, y), (x_test, y_test) = keras.datasets.fashion_mnist.load_data() # 返回numpy\n",
    "print(f\"x shape:{x.shape}, y shape:{y.shape}\")\n",
    "\n",
    "def preprocess(x, y):\n",
    "\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x,y\n",
    "\n",
    "batchsz = 128\n",
    "db = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "db = db.map(preprocess).shuffle(10000).batch(batchsz)\n",
    "db_iter = iter(db)\n",
    "sample = next(db_iter)\n",
    "print('batch:', sample[0].shape, sample[1].shape)\n",
    "\n",
    "db_test = tf.data.Dataset.from_tensor_slices((x_test,y_test))\n",
    "db_test = db_test.map(preprocess).batch(batchsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               200960    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 244,522\n",
      "Trainable params: 244,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Dense(256, activation=tf.nn.relu), # [b, 784] => [b, 256]\n",
    "    layers.Dense(128, activation=tf.nn.relu), # [b, 256] => [b, 128]\n",
    "    layers.Dense(64, activation=tf.nn.relu), # [b, 128] => [b, 64]\n",
    "    layers.Dense(32, activation=tf.nn.relu), # [b, 64] => [b, 32]\n",
    "    layers.Dense(10) # [b, 32] => [b, 10], 330 = 32*10 + 10\n",
    "])\n",
    "model.build(input_shape=[None, 28*28])\n",
    "model.summary()\n",
    "# w = w - lr*grad\n",
    "optimizer = optimizers.Adam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.创建一个logs目录\n",
    "# 2.启动tensorboard服务：tensorboard --logdir logs\n",
    "# 3.网页查看localhost:6006\n",
    "# 4.构建summary\n",
    "import datetime\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = 'logs/' + current_time\n",
    "summary_writer = tf.summary.create_file_writer(log_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备绘图函数\n",
    "def plot_to_image(figure):\n",
    "  \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
    "  returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n",
    "  # Save the plot to a PNG in memory.\n",
    "  buf = io.BytesIO()\n",
    "  plt.savefig(buf, format='png')\n",
    "  # Closing the figure prevents it from being displayed directly inside\n",
    "  # the notebook.\n",
    "  plt.close(figure)\n",
    "  buf.seek(0)\n",
    "  # Convert PNG buffer to TF image\n",
    "  image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "  # Add the batch dimension\n",
    "  image = tf.expand_dims(image, 0)\n",
    "  return image\n",
    "\n",
    "def image_grid(images):\n",
    "  \"\"\"Return a 5x5 grid of the MNIST images as a matplotlib figure.\"\"\"\n",
    "  # Create a figure to contain the plot.\n",
    "  figure = plt.figure(figsize=(10,10))\n",
    "  for i in range(min(25, len(images))):# 每25张图片一组\n",
    "    # Start next subplot.\n",
    "    plt.subplot(5, 5, i + 1, title='name')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(images[i], cmap=plt.cm.binary)\n",
    "  \n",
    "  return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 loss: 0.38710206747055054 18.419403076171875\n",
      "0 100 loss: 0.3311314582824707 19.710655212402344\n",
      "0 200 loss: 0.39904487133026123 18.829082489013672\n",
      "0 300 loss: 0.43409693241119385 19.332101821899414\n",
      "0 400 loss: 0.4294698238372803 17.10940933227539\n",
      "0 test acc: 0.8643\n",
      "1 0 loss: 0.16124220192432404 18.43413543701172\n",
      "1 100 loss: 0.3001267611980438 23.568584442138672\n",
      "1 200 loss: 0.22718653082847595 21.661556243896484\n",
      "1 300 loss: 0.2219761461019516 29.63250160217285\n",
      "1 400 loss: 0.34581488370895386 23.02088165283203\n",
      "1 test acc: 0.8698\n",
      "2 0 loss: 0.3177177309989929 23.92734718322754\n",
      "2 100 loss: 0.2695233225822449 27.289321899414062\n",
      "2 200 loss: 0.26662853360176086 22.334402084350586\n",
      "2 300 loss: 0.2795843183994293 24.796428680419922\n",
      "2 400 loss: 0.2523311376571655 21.879959106445312\n",
      "2 test acc: 0.8715\n",
      "3 0 loss: 0.19346833229064941 24.951229095458984\n",
      "3 100 loss: 0.2666340470314026 32.21454620361328\n",
      "3 200 loss: 0.2490980625152588 32.00535583496094\n",
      "3 300 loss: 0.25985270738601685 26.90170669555664\n",
      "3 400 loss: 0.2778441905975342 28.919300079345703\n",
      "3 test acc: 0.8706\n",
      "4 0 loss: 0.22355645895004272 28.41939926147461\n",
      "4 100 loss: 0.32545074820518494 34.372894287109375\n",
      "4 200 loss: 0.25641220808029175 33.880218505859375\n",
      "4 300 loss: 0.23526886105537415 31.61304473876953\n",
      "4 400 loss: 0.23620915412902832 33.2220458984375\n",
      "4 test acc: 0.8804\n",
      "5 0 loss: 0.23307669162750244 31.93804931640625\n",
      "5 100 loss: 0.3194980025291443 29.662181854248047\n",
      "5 200 loss: 0.25956401228904724 33.35858154296875\n",
      "5 300 loss: 0.2147948443889618 32.856021881103516\n",
      "5 400 loss: 0.24621525406837463 31.93720245361328\n",
      "5 test acc: 0.8768\n",
      "6 0 loss: 0.20274223387241364 30.580839157104492\n",
      "6 100 loss: 0.2667149305343628 33.42516326904297\n",
      "6 200 loss: 0.2806316018104553 36.35100555419922\n",
      "6 300 loss: 0.3492620289325714 31.55065155029297\n",
      "6 400 loss: 0.20512451231479645 31.71192169189453\n",
      "6 test acc: 0.8848\n",
      "7 0 loss: 0.2806086838245392 34.06587600708008\n",
      "7 100 loss: 0.2423401176929474 44.3072509765625\n",
      "7 200 loss: 0.22299176454544067 43.2375602722168\n",
      "7 300 loss: 0.23309248685836792 47.613319396972656\n",
      "7 400 loss: 0.22460773587226868 36.08592224121094\n",
      "7 test acc: 0.8834\n",
      "8 0 loss: 0.2448720932006836 36.6759033203125\n",
      "8 100 loss: 0.2676616311073303 37.82046127319336\n",
      "8 200 loss: 0.20948193967342377 46.191734313964844\n",
      "8 300 loss: 0.25629767775535583 44.09877014160156\n",
      "8 400 loss: 0.27799633145332336 44.598724365234375\n",
      "8 test acc: 0.8829\n",
      "9 0 loss: 0.2924302816390991 49.4754638671875\n",
      "9 100 loss: 0.1834772825241089 48.08220672607422\n",
      "9 200 loss: 0.26716721057891846 52.711647033691406\n",
      "9 300 loss: 0.18393100798130035 44.00090789794922\n",
      "9 400 loss: 0.24569348990917206 43.19209289550781\n",
      "9 test acc: 0.8826\n",
      "10 0 loss: 0.15102095901966095 47.59782028198242\n",
      "10 100 loss: 0.1458396017551422 60.97240447998047\n",
      "10 200 loss: 0.21541298925876617 49.632347106933594\n",
      "10 300 loss: 0.25909873843193054 50.39252471923828\n",
      "10 400 loss: 0.34068793058395386 50.648372650146484\n",
      "10 test acc: 0.8893\n",
      "11 0 loss: 0.15062107145786285 51.03070068359375\n",
      "11 100 loss: 0.30415982007980347 71.2651596069336\n",
      "11 200 loss: 0.21094968914985657 63.76209259033203\n",
      "11 300 loss: 0.274941086769104 50.7020263671875\n",
      "11 400 loss: 0.1415657103061676 53.03274917602539\n",
      "11 test acc: 0.8906\n",
      "12 0 loss: 0.1651010811328888 54.05647659301758\n",
      "12 100 loss: 0.20581932365894318 73.29179382324219\n",
      "12 200 loss: 0.20864948630332947 67.04679870605469\n",
      "12 300 loss: 0.2720154821872711 63.28498077392578\n",
      "12 400 loss: 0.3905982971191406 62.36656188964844\n",
      "12 test acc: 0.8906\n",
      "13 0 loss: 0.22238169610500336 65.33293151855469\n",
      "13 100 loss: 0.163191020488739 69.02400207519531\n",
      "13 200 loss: 0.100739486515522 76.31785583496094\n",
      "13 300 loss: 0.18679291009902954 76.68954467773438\n",
      "13 400 loss: 0.2427530288696289 78.86491394042969\n",
      "13 test acc: 0.8875\n",
      "14 0 loss: 0.16961854696273804 69.27552795410156\n",
      "14 100 loss: 0.246191143989563 72.68517303466797\n",
      "14 200 loss: 0.22624261677265167 85.55404663085938\n",
      "14 300 loss: 0.24521805346012115 73.7616958618164\n",
      "14 400 loss: 0.2000357210636139 64.15205383300781\n",
      "14 test acc: 0.8843\n",
      "15 0 loss: 0.18428045511245728 59.962188720703125\n",
      "15 100 loss: 0.17273157835006714 71.74922180175781\n",
      "15 200 loss: 0.22135360538959503 77.76640319824219\n",
      "15 300 loss: 0.2797451615333557 83.62950897216797\n",
      "15 400 loss: 0.15333320200443268 85.7109375\n",
      "15 test acc: 0.8889\n",
      "16 0 loss: 0.1539565771818161 73.77149200439453\n",
      "16 100 loss: 0.19643478095531464 87.75350952148438\n",
      "16 200 loss: 0.20402637124061584 101.85896301269531\n",
      "16 300 loss: 0.1362224519252777 78.88243103027344\n",
      "16 400 loss: 0.13029050827026367 79.90892028808594\n",
      "16 test acc: 0.8835\n",
      "17 0 loss: 0.14591792225837708 84.49311065673828\n",
      "17 100 loss: 0.2620477080345154 78.20864868164062\n",
      "17 200 loss: 0.13628895580768585 99.8759765625\n",
      "17 300 loss: 0.2184859961271286 107.34986877441406\n",
      "17 400 loss: 0.1856626272201538 96.0306625366211\n",
      "17 test acc: 0.8867\n",
      "18 0 loss: 0.19554904103279114 98.99657440185547\n",
      "18 100 loss: 0.13081477582454681 109.90290832519531\n",
      "18 200 loss: 0.21544283628463745 82.30462646484375\n",
      "18 300 loss: 0.19322580099105835 90.56201171875\n",
      "18 400 loss: 0.16470739245414734 90.79252624511719\n",
      "18 test acc: 0.8887\n",
      "19 0 loss: 0.20252388715744019 113.83489227294922\n",
      "19 100 loss: 0.08678805828094482 88.1710433959961\n",
      "19 200 loss: 0.11056020855903625 113.54804992675781\n",
      "19 300 loss: 0.12341223657131195 114.35334777832031\n",
      "19 400 loss: 0.17486032843589783 81.14315795898438\n",
      "19 test acc: 0.8905\n",
      "20 0 loss: 0.20364302396774292 107.67841339111328\n",
      "20 100 loss: 0.18308252096176147 90.21338653564453\n",
      "20 200 loss: 0.13691508769989014 96.02777099609375\n",
      "20 300 loss: 0.14190763235092163 106.38081359863281\n",
      "20 400 loss: 0.12645940482616425 99.4532470703125\n",
      "20 test acc: 0.8914\n",
      "21 0 loss: 0.11188556253910065 120.21199035644531\n",
      "21 100 loss: 0.25618797540664673 118.41111755371094\n",
      "21 200 loss: 0.13674971461296082 112.20175170898438\n",
      "21 300 loss: 0.19800995290279388 107.44312286376953\n",
      "21 400 loss: 0.14526543021202087 117.71615600585938\n",
      "21 test acc: 0.8933\n",
      "22 0 loss: 0.11758317798376083 114.31100463867188\n",
      "22 100 loss: 0.18532943725585938 118.28480529785156\n",
      "22 200 loss: 0.07625226676464081 137.31993103027344\n",
      "22 300 loss: 0.12324638664722443 142.17398071289062\n",
      "22 400 loss: 0.14910569787025452 134.45892333984375\n",
      "22 test acc: 0.8939\n",
      "23 0 loss: 0.15266360342502594 115.0943603515625\n",
      "23 100 loss: 0.18478043377399445 107.69148254394531\n",
      "23 200 loss: 0.18804845213890076 131.45559692382812\n",
      "23 300 loss: 0.10697196424007416 133.2574462890625\n",
      "23 400 loss: 0.20001089572906494 105.04163360595703\n",
      "23 test acc: 0.8968\n",
      "24 0 loss: 0.08005143702030182 129.36083984375\n",
      "24 100 loss: 0.09540523588657379 120.85664367675781\n",
      "24 200 loss: 0.19755104184150696 136.68423461914062\n",
      "24 300 loss: 0.1745409369468689 117.00395965576172\n",
      "24 400 loss: 0.16062353551387787 127.64483642578125\n",
      "24 test acc: 0.8952\n",
      "25 0 loss: 0.12996798753738403 136.65586853027344\n",
      "25 100 loss: 0.08304312825202942 134.053955078125\n",
      "25 200 loss: 0.14793461561203003 173.0511474609375\n",
      "25 300 loss: 0.0867520123720169 133.97254943847656\n",
      "25 400 loss: 0.15144605934619904 124.51213073730469\n",
      "25 test acc: 0.8935\n",
      "26 0 loss: 0.1398642510175705 126.9742660522461\n",
      "26 100 loss: 0.1480645090341568 148.6344757080078\n",
      "26 200 loss: 0.21735036373138428 160.74459838867188\n",
      "26 300 loss: 0.14275769889354706 148.280517578125\n",
      "26 400 loss: 0.13534975051879883 120.75383758544922\n",
      "26 test acc: 0.8893\n",
      "27 0 loss: 0.11257115006446838 153.32369995117188\n",
      "27 100 loss: 0.16556015610694885 165.85556030273438\n",
      "27 200 loss: 0.1110212504863739 135.45571899414062\n",
      "27 300 loss: 0.09954869002103806 173.59141540527344\n",
      "27 400 loss: 0.10832273960113525 168.6520233154297\n",
      "27 test acc: 0.8952\n",
      "28 0 loss: 0.051978699862957 135.6705780029297\n",
      "28 100 loss: 0.1105462908744812 140.38143920898438\n",
      "28 200 loss: 0.11355197429656982 162.69097900390625\n",
      "28 300 loss: 0.06180668994784355 163.01510620117188\n",
      "28 400 loss: 0.2048216462135315 136.79757690429688\n",
      "28 test acc: 0.8899\n",
      "29 0 loss: 0.07564263045787811 143.7750244140625\n",
      "29 100 loss: 0.12582693994045258 158.89944458007812\n",
      "29 200 loss: 0.11458601802587509 137.67340087890625\n",
      "29 300 loss: 0.06133890524506569 161.3539276123047\n",
      "29 400 loss: 0.1104804128408432 138.60031127929688\n",
      "29 test acc: 0.8912\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "\n",
    "\n",
    "    for step, (x,y) in enumerate(db):\n",
    "\n",
    "        # x: [b, 28, 28] => [b, 784]\n",
    "        # y: [b]\n",
    "        x = tf.reshape(x, [-1, 28*28])\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # [b, 784] => [b, 10]\n",
    "            logits = model(x)\n",
    "            y_onehot = tf.one_hot(y, depth=10)\n",
    "            # [b]\n",
    "            loss_mse = tf.reduce_mean(tf.losses.MSE(y_onehot, logits))\n",
    "            loss_ce = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)\n",
    "            loss_ce = tf.reduce_mean(loss_ce)\n",
    "\n",
    "        grads = tape.gradient(loss_ce, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(epoch, step, 'loss:', float(loss_ce), float(loss_mse))\n",
    "            # 5.写入summary\n",
    "            with summary_writer.as_default(): \n",
    "                tf.summary.scalar('train-loss', float(loss_ce), step=step) \n",
    "\n",
    "\n",
    "    # test\n",
    "    total_correct = 0\n",
    "    total_num = 0\n",
    "    for x,y in db_test:\n",
    "\n",
    "        # x: [b, 28, 28] => [b, 784]\n",
    "        # y: [b]\n",
    "        x = tf.reshape(x, [-1, 28*28])\n",
    "        # [b, 10]\n",
    "        logits = model(x)\n",
    "        # logits => prob, [b, 10]\n",
    "        prob = tf.nn.softmax(logits, axis=1)\n",
    "        # [b, 10] => [b], int64\n",
    "        pred = tf.argmax(prob, axis=1)\n",
    "        pred = tf.cast(pred, dtype=tf.int32)\n",
    "        # pred:[b]\n",
    "        # y: [b]\n",
    "        # correct: [b], True: equal, False: not equal\n",
    "        correct = tf.equal(pred, y)\n",
    "        correct = tf.reduce_sum(tf.cast(correct, dtype=tf.int32))\n",
    "\n",
    "        total_correct += int(correct)\n",
    "        total_num += x.shape[0]\n",
    "\n",
    "    acc = total_correct / total_num\n",
    "    print(epoch, 'test acc:', acc)\n",
    "\n",
    "    # 5.写入summary\n",
    "    val_images = x[:25] # 每25张图片一组\n",
    "    val_images = tf.reshape(val_images, [-1, 28, 28, 1])\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('test-acc', float(total_correct/total_num), step=step)\n",
    "        tf.summary.image(\"val-onebyone-images:\", val_images, max_outputs=25, step=step)\n",
    "        \n",
    "        val_images = tf.reshape(val_images, [-1, 28, 28])\n",
    "        figure  = image_grid(val_images)\n",
    "        tf.summary.image('val-images:', plot_to_image(figure), step=step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bafc338abc57834bacf3c306014ce0c45f0aeaf201b721c6f2bfe17ccf7009fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
