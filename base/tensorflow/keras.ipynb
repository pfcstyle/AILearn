{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape:(60000, 28, 28), y shape:(60000,)\n",
      "batch: (128, 784) (128, 10)\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import Sequential, layers, metrics, optimizers\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "(x, y), (x_test, y_test) = keras.datasets.fashion_mnist.load_data() # 返回numpy\n",
    "print(f\"x shape:{x.shape}, y shape:{y.shape}\")\n",
    "\n",
    "def preprocess(x, y):\n",
    "\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
    "    x = tf.reshape(x, [28*28])\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    y = tf.one_hot(y, depth=10)\n",
    "    return x,y\n",
    "\n",
    "batchsz = 128\n",
    "db = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "db = db.map(preprocess).shuffle(10000).batch(batchsz)\n",
    "db_iter = iter(db)\n",
    "sample = next(db_iter)\n",
    "print('batch:', sample[0].shape, sample[1].shape)\n",
    "\n",
    "db_test = tf.data.Dataset.from_tensor_slices((x_test,y_test))\n",
    "db_test = db_test.map(preprocess).batch(batchsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([784, 512]), TensorShape([512]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.normal([4, 784])\n",
    "# Dense是全连接层\n",
    "net = keras.layers.Dense(512) # 声明时并不会创建，在build时才会，因为不知道input的shape\n",
    "# net.bias, net.weights  空的\n",
    "net.build(input_shape=(None, 784))# 指定最后一维即可\n",
    "net.bias, net.weights # 有值了\n",
    "out = net(x)\n",
    "\n",
    "out.shape\n",
    "\n",
    "# w b\n",
    "net.kernel.shape, net.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 256)               200960    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 244,522\n",
      "Trainable params: 244,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "dense_1/kernel:0 (784, 256)\n",
      "dense_1/bias:0 (256,)\n",
      "dense_2/kernel:0 (256, 128)\n",
      "dense_2/bias:0 (128,)\n",
      "dense_3/kernel:0 (128, 64)\n",
      "dense_3/bias:0 (64,)\n",
      "dense_4/kernel:0 (64, 32)\n",
      "dense_4/bias:0 (32,)\n",
      "dense_5/kernel:0 (32, 10)\n",
      "dense_5/bias:0 (10,)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Dense(256, activation=tf.nn.relu), # [b, 784] => [b, 256]\n",
    "    layers.Dense(128, activation=tf.nn.relu), # [b, 256] => [b, 128]\n",
    "    layers.Dense(64, activation=tf.nn.relu), # [b, 128] => [b, 64]\n",
    "    layers.Dense(32, activation=tf.nn.relu), # [b, 64] => [b, 32]\n",
    "    layers.Dense(10) # [b, 32] => [b, 10], 330 = 32*10 + 10\n",
    "])\n",
    "model.build(input_shape=[None, 28*28])\n",
    "model.summary()\n",
    "# same to model.summary()\n",
    "# 可以遍历到所有的参数\n",
    "for p in model.trainable_variables:\n",
    "\tprint(p.name, p.shape)\n",
    "# w = w - lr*grad\n",
    "optimizer = optimizers.Adam(learning_rate=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics使用\n",
    "# 1. build meter\n",
    "acc_meter = metrics.Accuracy()\n",
    "loss_meter = metrics.Mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 loss: 2.3338937759399414 0.17064492404460907 2.3338938\n",
      "0 100 loss: 0.46646273136138916 23.90788459777832 0.80284345\n",
      "0 200 loss: 0.5061992406845093 25.445873260498047 0.51248395\n",
      "0 300 loss: 0.43094614148139954 27.841815948486328 0.46819782\n",
      "0 400 loss: 0.6341909766197205 26.187862396240234 0.4508484\n",
      "0 test acc: 0.8465 0.8465\n",
      "1 0 loss: 0.38558927178382874 28.730772018432617 0.42150962\n",
      "1 100 loss: 0.4585285484790802 29.783458709716797 0.3896769\n",
      "1 200 loss: 0.3725021183490753 31.84864044189453 0.37938073\n",
      "1 300 loss: 0.2814905047416687 25.101472854614258 0.37032977\n",
      "1 400 loss: 0.3613806366920471 27.357162475585938 0.3632713\n",
      "1 test acc: 0.8608 0.8608\n",
      "2 0 loss: 0.27347150444984436 30.221534729003906 0.3673601\n",
      "2 100 loss: 0.36720848083496094 36.19776916503906 0.3359236\n",
      "2 200 loss: 0.3515879511833191 31.31053352355957 0.3502842\n",
      "2 300 loss: 0.3477889597415924 38.70899200439453 0.32736564\n",
      "2 400 loss: 0.285636842250824 30.92763328552246 0.3302698\n",
      "2 test acc: 0.8761 0.8761\n",
      "3 0 loss: 0.27685847878456116 35.55518341064453 0.33607328\n",
      "3 100 loss: 0.3623293340206146 33.67559051513672 0.3132037\n",
      "3 200 loss: 0.306840717792511 40.34260940551758 0.31253818\n",
      "3 300 loss: 0.3503448963165283 39.819679260253906 0.2937108\n",
      "3 400 loss: 0.3554014563560486 39.78065490722656 0.3098215\n",
      "3 test acc: 0.8758 0.8758\n",
      "4 0 loss: 0.2492610067129135 39.543739318847656 0.30867788\n",
      "4 100 loss: 0.31595146656036377 43.41156005859375 0.28516412\n",
      "4 200 loss: 0.172145277261734 45.78266143798828 0.3013277\n",
      "4 300 loss: 0.20018446445465088 50.39481735229492 0.28191492\n",
      "4 400 loss: 0.2869689464569092 44.448631286621094 0.29345995\n",
      "4 test acc: 0.863 0.863\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "\n",
    "\n",
    "    for step, (x,y) in enumerate(db):\n",
    "\n",
    "        # x: [b, 28, 28] => [b, 784]\n",
    "        # y: [b]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # [b, 784] => [b, 10]\n",
    "            logits = model(x)\n",
    "            # [b]\n",
    "            loss_mse = tf.reduce_mean(tf.losses.MSE(y, logits))\n",
    "            loss_ce = tf.losses.categorical_crossentropy(y, logits, from_logits=True)\n",
    "            loss_ce = tf.reduce_mean(loss_ce)\n",
    "            # 2. metrics使用 update data\n",
    "            loss_meter.update_state(loss_ce)\n",
    "\n",
    "        grads = tape.gradient(loss_ce, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(epoch, step, 'loss:', float(loss_ce), float(loss_mse), loss_meter.result().numpy())\n",
    "            # 3. metrics使用 重置states\n",
    "            loss_meter.reset_states()\n",
    "\n",
    "\n",
    "    # test\n",
    "    total_correct = 0\n",
    "    total_num = 0\n",
    "    # 3. metrics使用 重置states\n",
    "    acc_meter.reset_states()\n",
    "    for x,y in db_test:\n",
    "        # [b, 10]\n",
    "        logits = model(x)\n",
    "        # logits => prob, [b, 10]\n",
    "        prob = tf.nn.softmax(logits, axis=1)\n",
    "        # [b, 10] => [b], int64\n",
    "        pred = tf.argmax(prob, axis=1)\n",
    "        pred = tf.cast(pred, dtype=tf.int32)\n",
    "        # pred:[b]\n",
    "        # y: [b]\n",
    "        # correct: [b], True: equal, False: not equal\n",
    "        y = tf.argmax(y, axis=1)\n",
    "        y = tf.cast(y, dtype=tf.int32)\n",
    "        correct = tf.equal(pred, y)\n",
    "        correct = tf.reduce_sum(tf.cast(correct, dtype=tf.int32))\n",
    "\n",
    "        total_correct += int(correct)\n",
    "        total_num += x.shape[0]\n",
    "        acc_meter.update_state(y, pred)\n",
    "\n",
    "    acc = total_correct / total_num\n",
    "    print(epoch, 'test acc:', acc, acc_meter.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.2807 - accuracy: 0.8956\n",
      "Epoch 2/6\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.2668 - accuracy: 0.9011 - val_loss: 0.3314 - val_accuracy: 0.8830\n",
      "Epoch 3/6\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2544 - accuracy: 0.9034\n",
      "Epoch 4/6\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.2459 - accuracy: 0.9077 - val_loss: 0.3390 - val_accuracy: 0.8814\n",
      "Epoch 5/6\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.2349 - accuracy: 0.9109\n",
      "Epoch 6/6\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.2249 - accuracy: 0.9151 - val_loss: 0.3299 - val_accuracy: 0.8859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a28ab9a1c0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用compile和fit进行训练\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "# 同时设置测试数据集，指定验证频率，每2个epoch做一次validation\n",
    "# model内部会自动调用每一层的call  model.__call__() -> layer.call()\n",
    "model.fit(db, epochs=6, validation_data=db_test, validation_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3299 - accuracy: 0.8859\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "tf.Tensor(\n",
      "[9 2 1 1 6 1 4 6 5 7 4 5 5 3 4 1 2 2 8 0 2 5 7 5 1 2 6 0 9 3 8 8 3 3 8 0 7\n",
      " 5 7 9 0 1 6 7 6 7 2 1 2 6 4 2 5 8 2 2 8 6 8 0 7 7 8 5 1 1 6 4 7 8 7 0 2 6\n",
      " 2 3 1 2 8 4 1 8 5 9 5 0 3 2 0 2 5 3 6 7 1 8 0 1 2 2 3 6 7 2 7 8 5 7 9 4 2\n",
      " 5 7 0 5 2 8 6 7 8 0 0 9 9 3 0 8 2], shape=(128,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[9 2 1 1 6 1 4 6 5 7 4 5 7 3 4 1 2 4 8 0 2 5 7 9 1 4 6 0 9 3 8 8 3 3 8 0 7\n",
      " 5 7 9 6 1 3 7 6 7 2 1 2 2 4 4 5 8 2 2 8 4 8 0 7 7 8 5 1 1 2 3 9 8 7 0 2 6\n",
      " 2 3 1 2 8 4 1 8 5 9 5 0 3 2 0 6 5 3 6 7 1 8 0 1 4 2 3 6 7 2 7 8 5 9 9 4 2\n",
      " 5 7 0 5 2 8 6 7 8 0 0 9 9 3 0 8 4], shape=(128,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# 测试  这和上面fit时指定validation是一样的，但是fit指定validation可以让fit提前结束\n",
    "model.evaluate(db_test)\n",
    "\n",
    "sample = next(iter(db_test))\n",
    "x = sample[0]\n",
    "y = sample[1] # one-hot\n",
    "# model(x)与model.predict(x)意思一样\n",
    "pred = model.predict(x) # [b, 10]\n",
    "# convert back to number \n",
    "y = tf.argmax(y, axis=1)\n",
    "pred = tf.argmax(pred, axis=1)\n",
    "\n",
    "print(pred)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom layer/model\n",
    "# keras.Model与keras.layers.layer类似，都需要继承父类，覆盖__init__, call方法。Model多出了compile/fit/evaluate/predict函数\n",
    "\n",
    "# Custom layer\n",
    "class MyDense(layers.Layer):\n",
    "\n",
    "\tdef __init__(self, inp_dim, outp_dim):\n",
    "\t\tsuper(MyDense, self).__init__() # 必须写\n",
    "\t\tself.inp_dim = inp_dim\n",
    "\t\tself.outp_dim = outp_dim\n",
    "        # w, b自定义，注意由于框架集成原因，kernel，bias定义时不能用tf.variable等直接定义\n",
    "\t\tself.kernel = self.add_weight('w', [inp_dim, outp_dim])\n",
    "\t\tself.bias = self.add_weight('b', [outp_dim])\n",
    "\n",
    "\tdef call(self, inputs, training=None):\n",
    "\n",
    "\t\tout = inputs @ self.kernel + self.bias\n",
    "\n",
    "\t\treturn out \n",
    "\n",
    "\tdef get_config(self):\n",
    "\t\t# have to define get_config to be able to use model_from_json\n",
    "\t\tconfig = {\n",
    "\t\t\t'inp_dim': self.inp_dim,\n",
    "\t\t\t'outp_dim': self.outp_dim\n",
    "\t\t}\n",
    "\t\tbase_config = super().get_config()\n",
    "\t\treturn dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# custom model\n",
    "class MyModel(keras.Model):\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(MyModel, self).__init__()\n",
    "        # 构建model结构\n",
    "\t\tself.fc1 = MyDense(28*28, 256)\n",
    "\t\tself.fc2 = MyDense(256, 128)\n",
    "\t\tself.fc3 = MyDense(128, 64)\n",
    "\t\tself.fc4 = MyDense(64, 32)\n",
    "\t\tself.fc5 = MyDense(32, 10)\n",
    "\n",
    "\tdef call(self, inputs, training=None):\n",
    "        # 实现x在layers间的传递过程\n",
    "\t\tx = self.fc1(inputs)\n",
    "\t\tx = tf.nn.relu(x)\n",
    "\t\tx = self.fc2(x)\n",
    "\t\tx = tf.nn.relu(x)\n",
    "\t\tx = self.fc3(x)\n",
    "\t\tx = tf.nn.relu(x)\n",
    "\t\tx = self.fc4(x)\n",
    "\t\tx = tf.nn.relu(x)\n",
    "\t\tx = self.fc5(x) \n",
    "\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.5668 - accuracy: 0.8001\n",
      "Epoch 2/6\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.3804 - accuracy: 0.8625 - val_loss: 0.3901 - val_accuracy: 0.8576\n",
      "Epoch 3/6\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.3392 - accuracy: 0.8757\n",
      "Epoch 4/6\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.3108 - accuracy: 0.8847 - val_loss: 0.3535 - val_accuracy: 0.8745\n",
      "Epoch 5/6\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.2933 - accuracy: 0.8912\n",
      "Epoch 6/6\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.2771 - accuracy: 0.8962 - val_loss: 0.3322 - val_accuracy: 0.8816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a28abcd910>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyModel()\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "# 同时设置测试数据集，指定验证频率，每2个epoch做一次validation\n",
    "# model内部会自动调用每一层的call  model.__call__() -> layer.call()\n",
    "model.fit(db, epochs=6, validation_data=db_test, validation_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 0s 3ms/step - loss: 0.3322 - accuracy: 0.8816\n",
      "saved weights.\n",
      "loaded weights!\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3322 - accuracy: 0.8816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.33221739530563354, 0.881600022315979]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型保存与加载  遵循ONNX标准\n",
    "# 权重保存与加载\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "dir_path = 'weights/fashion_minist'\n",
    "if os.path.exists(dir_path):\n",
    "    shutil.rmtree(dir_path)\n",
    "# os.makedirs(dir_path)\n",
    "\n",
    "weight_path = os.path.join(dir_path, 'weights.ckpt')\n",
    "model.save_weights(weight_path)\n",
    "model.evaluate(db_test)\n",
    "print('saved weights.')\n",
    "del model\n",
    "\n",
    "model = MyModel()\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "model.load_weights(weight_path)\n",
    "print('loaded weights!')\n",
    "model.evaluate(db_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.5429 - accuracy: 0.8086\n",
      "Epoch 2/6\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.3738 - accuracy: 0.8640 - val_loss: 0.3884 - val_accuracy: 0.8592\n",
      "Epoch 3/6\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.3337 - accuracy: 0.8770\n",
      "Epoch 4/6\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.3081 - accuracy: 0.8868 - val_loss: 0.3892 - val_accuracy: 0.8589\n",
      "Epoch 5/6\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.2914 - accuracy: 0.8909\n",
      "Epoch 6/6\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.2790 - accuracy: 0.8948 - val_loss: 0.3455 - val_accuracy: 0.8757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a29a2f0dc0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 尝试使用自定义layer\n",
    "\n",
    "class MyLayer(layers.Layer):\n",
    "\n",
    "\tdef __init__(self, inp_dim, outp_dim, **kwargs):\n",
    "\t\tsuper(MyLayer, self).__init__() # 必须写\n",
    "\t\tself.inp_dim = inp_dim\n",
    "\t\tself.outp_dim = outp_dim\n",
    "        # w, b自定义，注意由于框架集成原因，kernel，bias定义时不能用tf.variable等直接定义\n",
    "\t\tself.kernel = self.add_weight('w', [inp_dim, outp_dim])\n",
    "\t\tself.bias = self.add_weight('b', [outp_dim])\n",
    "\n",
    "\tdef call(self, inputs, training=None):\n",
    "\n",
    "\t\tout = inputs @ self.kernel + self.bias\n",
    "\n",
    "\t\treturn out \n",
    "\n",
    "\tdef get_config(self):\n",
    "\t\t# have to define get_config to be able to use model_from_json\n",
    "\t\tconfig = {\n",
    "\t\t\t'inp_dim': self.inp_dim,\n",
    "\t\t\t'outp_dim': self.outp_dim\n",
    "\t\t}\n",
    "\t\tbase_config = super().get_config()\n",
    "\t\treturn dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "model = Sequential([\n",
    "    MyLayer(28*28, 256),\n",
    "    layers.ReLU(),\n",
    "    # layers.Dense(256, activation=tf.nn.relu), # [b, 784] => [b, 256]\n",
    "    layers.Dense(128, activation=tf.nn.relu), # [b, 256] => [b, 128]\n",
    "    layers.Dense(64, activation=tf.nn.relu), # [b, 128] => [b, 64]\n",
    "    layers.Dense(32, activation=tf.nn.relu), # [b, 64] => [b, 32]\n",
    "    layers.Dense(10) # [b, 32] => [b, 10], 330 = 32*10 + 10\n",
    "])\n",
    "\n",
    "# 使用compile和fit进行训练\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "# 同时设置测试数据集，指定验证频率，每2个epoch做一次validation\n",
    "# model内部会自动调用每一层的call  model.__call__() -> layer.call()\n",
    "model.fit(db, epochs=6, validation_data=db_test, validation_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved total model.\n",
      "loaded model from file.\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3455 - accuracy: 0.8757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.34546780586242676, 0.8756999969482422]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "dir_path = 'models/fashion_minist'\n",
    "if os.path.exists(dir_path):\n",
    "    shutil.rmtree(dir_path)\n",
    "# os.makedirs(dir_path)\n",
    "\n",
    "model_path = os.path.join(dir_path, 'model.h5')\n",
    "model.save(model_path) # 默认h5格式不支持自定义model, 可以保存weights或者指定tf格式:save_format=\"tf\"\n",
    "print('saved total model.')\n",
    "del model\n",
    "\n",
    "print('loaded model from file.')\n",
    "model = tf.keras.models.load_model(model_path, compile=False, custom_objects={'MyLayer': MyLayer})\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "model.evaluate(db_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('new_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d58ff1fa528449bec86ac3b87e5b9edec9a0f46f3f9c706338d5512923abfa3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
