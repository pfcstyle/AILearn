{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist\n",
    "(x, y), (x_test, y_test) = keras.datasets.mnist.load_data() # 返回numpy\n",
    "x.shape\n",
    "y.shape\n",
    "x.min()\n",
    "y_onehot = tf.one_hot(y, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape:(60000, 28, 28), y shape:(60000,)\n"
     ]
    }
   ],
   "source": [
    "# fashion mnist\n",
    "(x, y), (x_test, y_test) = keras.datasets.fashion_mnist.load_data() # 返回numpy\n",
    "print(f\"x shape:{x.shape}, y shape:{y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: (128, 28, 28) (128,)\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, optimizers, Sequential, metrics\n",
    "\n",
    "def preprocess(x, y):\n",
    "\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x,y\n",
    "\n",
    "batchsz = 128\n",
    "db = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "db = db.map(preprocess).shuffle(10000).batch(batchsz)\n",
    "db_iter = iter(db)\n",
    "sample = next(db_iter)\n",
    "print('batch:', sample[0].shape, sample[1].shape)\n",
    "\n",
    "db_test = tf.data.Dataset.from_tensor_slices((x_test,y_test))\n",
    "db_test = db_test.map(preprocess).batch(batchsz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               200960    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 244,522\n",
      "Trainable params: 244,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Dense(256, activation=tf.nn.relu), # [b, 784] => [b, 256]\n",
    "    layers.Dense(128, activation=tf.nn.relu), # [b, 256] => [b, 128]\n",
    "    layers.Dense(64, activation=tf.nn.relu), # [b, 128] => [b, 64]\n",
    "    layers.Dense(32, activation=tf.nn.relu), # [b, 64] => [b, 32]\n",
    "    layers.Dense(10) # [b, 32] => [b, 10], 330 = 32*10 + 10\n",
    "])\n",
    "model.build(input_shape=[None, 28*28])\n",
    "model.summary()\n",
    "# w = w - lr*grad\n",
    "# Adam是最好的优化器，如果你想花更少的时间更有效地训练神经网络那就用Adam。\n",
    "# 对于稀疏的数据，使用动态的学习率。\n",
    "# 如果你想用梯度下降算法，mini-batch梯度下降是最好的选择。\n",
    "optimizer = optimizers.Adam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 loss: 2.365816116333008 0.17254051566123962\n",
      "0 100 loss: 0.6091817617416382 17.28742027282715\n",
      "0 200 loss: 0.4815180003643036 19.865001678466797\n",
      "0 300 loss: 0.4163750410079956 20.360986709594727\n",
      "0 400 loss: 0.3302662670612335 23.38900375366211\n",
      "0 test acc: 0.8344\n",
      "1 0 loss: 0.4726368188858032 23.090065002441406\n",
      "1 100 loss: 0.45495569705963135 27.5794677734375\n",
      "1 200 loss: 0.33877336978912354 26.329254150390625\n",
      "1 300 loss: 0.4583154618740082 22.437856674194336\n",
      "1 400 loss: 0.45800724625587463 24.718666076660156\n",
      "1 test acc: 0.8672\n",
      "2 0 loss: 0.365939736366272 25.27383041381836\n",
      "2 100 loss: 0.44438666105270386 23.356464385986328\n",
      "2 200 loss: 0.4254318177700043 31.04400634765625\n",
      "2 300 loss: 0.2957124412059784 35.90864944458008\n",
      "2 400 loss: 0.2693067789077759 30.904319763183594\n",
      "2 test acc: 0.869\n",
      "3 0 loss: 0.17273060977458954 40.43163299560547\n",
      "3 100 loss: 0.4020720422267914 36.184425354003906\n",
      "3 200 loss: 0.21347549557685852 36.93004608154297\n",
      "3 300 loss: 0.3364000916481018 32.04872131347656\n",
      "3 400 loss: 0.3213058114051819 31.74980926513672\n",
      "3 test acc: 0.8675\n",
      "4 0 loss: 0.33973509073257446 38.115234375\n",
      "4 100 loss: 0.33048760890960693 41.561805725097656\n",
      "4 200 loss: 0.30698978900909424 39.498191833496094\n",
      "4 300 loss: 0.3405410945415497 46.98957061767578\n",
      "4 400 loss: 0.2763557434082031 42.23680114746094\n",
      "4 test acc: 0.871\n",
      "5 0 loss: 0.2225680649280548 40.75090789794922\n",
      "5 100 loss: 0.3029043674468994 46.24860382080078\n",
      "5 200 loss: 0.16844414174556732 47.886192321777344\n",
      "5 300 loss: 0.228928804397583 43.370243072509766\n",
      "5 400 loss: 0.3598554730415344 48.689327239990234\n",
      "5 test acc: 0.8766\n",
      "6 0 loss: 0.18904343247413635 44.0230712890625\n",
      "6 100 loss: 0.35786136984825134 51.63713836669922\n",
      "6 200 loss: 0.3834497332572937 53.9491081237793\n",
      "6 300 loss: 0.33739686012268066 43.508453369140625\n",
      "6 400 loss: 0.28523558378219604 45.891319274902344\n",
      "6 test acc: 0.8787\n",
      "7 0 loss: 0.20041882991790771 49.257164001464844\n",
      "7 100 loss: 0.2973763346672058 45.70787811279297\n",
      "7 200 loss: 0.22484320402145386 57.86435317993164\n",
      "7 300 loss: 0.29099851846694946 65.0137939453125\n",
      "7 400 loss: 0.28199678659439087 48.19097137451172\n",
      "7 test acc: 0.8803\n",
      "8 0 loss: 0.2580667734146118 48.31294250488281\n",
      "8 100 loss: 0.23011639714241028 54.14655685424805\n",
      "8 200 loss: 0.23159298300743103 67.18221282958984\n",
      "8 300 loss: 0.3290722668170929 58.02070617675781\n",
      "8 400 loss: 0.3072890639305115 57.67041015625\n",
      "8 test acc: 0.875\n",
      "9 0 loss: 0.24202986061573029 55.88456726074219\n",
      "9 100 loss: 0.279371440410614 59.62992858886719\n",
      "9 200 loss: 0.18881380558013916 70.79898071289062\n",
      "9 300 loss: 0.1599961519241333 60.46430587768555\n",
      "9 400 loss: 0.18403026461601257 72.23087310791016\n",
      "9 test acc: 0.8778\n",
      "10 0 loss: 0.25551095604896545 72.97883605957031\n",
      "10 100 loss: 0.1741401106119156 64.45285034179688\n",
      "10 200 loss: 0.21514524519443512 72.46815490722656\n",
      "10 300 loss: 0.2039620280265808 63.13438034057617\n",
      "10 400 loss: 0.2167508900165558 66.22737884521484\n",
      "10 test acc: 0.8816\n",
      "11 0 loss: 0.2354186326265335 70.46660614013672\n",
      "11 100 loss: 0.16197416186332703 80.65858459472656\n",
      "11 200 loss: 0.19319814443588257 73.4976806640625\n",
      "11 300 loss: 0.1420162469148636 69.42869567871094\n",
      "11 400 loss: 0.1546553075313568 77.08956909179688\n",
      "11 test acc: 0.8884\n",
      "12 0 loss: 0.2795981168746948 80.26791381835938\n",
      "12 100 loss: 0.23466819524765015 85.5215072631836\n",
      "12 200 loss: 0.2252117097377777 94.34977722167969\n",
      "12 300 loss: 0.17941886186599731 84.09060668945312\n",
      "12 400 loss: 0.2012510597705841 83.87811279296875\n",
      "12 test acc: 0.8752\n",
      "13 0 loss: 0.165531188249588 83.50640869140625\n",
      "13 100 loss: 0.27092236280441284 83.75952911376953\n",
      "13 200 loss: 0.16324593126773834 79.50159454345703\n",
      "13 300 loss: 0.09029997885227203 104.27940368652344\n",
      "13 400 loss: 0.22803407907485962 90.5166015625\n",
      "13 test acc: 0.8885\n",
      "14 0 loss: 0.1179356649518013 78.00628662109375\n",
      "14 100 loss: 0.3467824459075928 73.24334716796875\n",
      "14 200 loss: 0.2363690584897995 107.6055679321289\n",
      "14 300 loss: 0.2358618974685669 90.43582916259766\n",
      "14 400 loss: 0.27361220121383667 95.09344482421875\n",
      "14 test acc: 0.8878\n",
      "15 0 loss: 0.231400728225708 84.38795471191406\n",
      "15 100 loss: 0.1718803495168686 114.45042419433594\n",
      "15 200 loss: 0.1581433117389679 113.75166320800781\n",
      "15 300 loss: 0.12118751555681229 101.55299377441406\n",
      "15 400 loss: 0.21515652537345886 95.92218017578125\n",
      "15 test acc: 0.8878\n",
      "16 0 loss: 0.19021141529083252 94.48591613769531\n",
      "16 100 loss: 0.18360655009746552 123.55442810058594\n",
      "16 200 loss: 0.16885489225387573 136.97799682617188\n",
      "16 300 loss: 0.17774438858032227 117.73594665527344\n",
      "16 400 loss: 0.2312678098678589 119.56312561035156\n",
      "16 test acc: 0.8903\n",
      "17 0 loss: 0.2084287852048874 126.95951080322266\n",
      "17 100 loss: 0.1504170447587967 115.72953796386719\n",
      "17 200 loss: 0.16267628967761993 120.20173645019531\n",
      "17 300 loss: 0.12827423214912415 121.91307067871094\n",
      "17 400 loss: 0.2442367970943451 128.6348876953125\n",
      "17 test acc: 0.8862\n",
      "18 0 loss: 0.18402448296546936 111.50347900390625\n",
      "18 100 loss: 0.14127907156944275 142.4921112060547\n",
      "18 200 loss: 0.1926443874835968 131.91571044921875\n",
      "18 300 loss: 0.13586369156837463 130.73202514648438\n",
      "18 400 loss: 0.18747937679290771 123.43968963623047\n",
      "18 test acc: 0.8892\n",
      "19 0 loss: 0.18119332194328308 102.48058319091797\n",
      "19 100 loss: 0.189419686794281 128.0985107421875\n",
      "19 200 loss: 0.14992055296897888 122.59349060058594\n",
      "19 300 loss: 0.1988849937915802 159.19381713867188\n",
      "19 400 loss: 0.15239325165748596 139.95956420898438\n",
      "19 test acc: 0.8867\n",
      "20 0 loss: 0.1633216291666031 126.8113021850586\n",
      "20 100 loss: 0.14686568081378937 127.96324157714844\n",
      "20 200 loss: 0.13012976944446564 166.90965270996094\n",
      "20 300 loss: 0.2602058947086334 157.0546875\n",
      "20 400 loss: 0.14243565499782562 152.79824829101562\n",
      "20 test acc: 0.8933\n",
      "21 0 loss: 0.12239891290664673 127.63899230957031\n",
      "21 100 loss: 0.11245885491371155 122.01612091064453\n",
      "21 200 loss: 0.14454086124897003 161.16378784179688\n",
      "21 300 loss: 0.14145323634147644 124.78038024902344\n",
      "21 400 loss: 0.12961462140083313 167.56411743164062\n",
      "21 test acc: 0.8948\n",
      "22 0 loss: 0.23126910626888275 173.13754272460938\n",
      "22 100 loss: 0.2526325583457947 136.39736938476562\n",
      "22 200 loss: 0.11136197298765182 184.25408935546875\n",
      "22 300 loss: 0.17177000641822815 161.87294006347656\n",
      "22 400 loss: 0.13118377327919006 155.90338134765625\n",
      "22 test acc: 0.8891\n",
      "23 0 loss: 0.1146063357591629 174.25967407226562\n",
      "23 100 loss: 0.11704502999782562 167.96746826171875\n",
      "23 200 loss: 0.11433633416891098 183.80401611328125\n",
      "23 300 loss: 0.21803027391433716 158.19967651367188\n",
      "23 400 loss: 0.16892758011817932 194.76084899902344\n",
      "23 test acc: 0.8912\n",
      "24 0 loss: 0.17269760370254517 184.03988647460938\n",
      "24 100 loss: 0.18615128099918365 177.47769165039062\n",
      "24 200 loss: 0.1121005043387413 179.13134765625\n",
      "24 300 loss: 0.20946577191352844 222.11465454101562\n",
      "24 400 loss: 0.09754591435194016 197.77598571777344\n",
      "24 test acc: 0.8892\n",
      "25 0 loss: 0.2067996859550476 189.36514282226562\n",
      "25 100 loss: 0.10723310708999634 263.69158935546875\n",
      "25 200 loss: 0.07067792117595673 193.82151794433594\n",
      "25 300 loss: 0.1193496361374855 214.03396606445312\n",
      "25 400 loss: 0.1586083173751831 226.46507263183594\n",
      "25 test acc: 0.8931\n",
      "26 0 loss: 0.13784542679786682 219.78875732421875\n",
      "26 100 loss: 0.10875139385461807 212.825927734375\n",
      "26 200 loss: 0.09205231815576553 216.61972045898438\n",
      "26 300 loss: 0.13390909135341644 205.76995849609375\n",
      "26 400 loss: 0.09694628417491913 199.07882690429688\n",
      "26 test acc: 0.8873\n",
      "27 0 loss: 0.12577411532402039 195.39178466796875\n",
      "27 100 loss: 0.11962513625621796 219.69375610351562\n",
      "27 200 loss: 0.13266544044017792 224.66232299804688\n",
      "27 300 loss: 0.11684102565050125 200.4130859375\n",
      "27 400 loss: 0.10529830306768417 219.83665466308594\n",
      "27 test acc: 0.8852\n",
      "28 0 loss: 0.18832245469093323 175.80325317382812\n",
      "28 100 loss: 0.08439294993877411 181.9547882080078\n",
      "28 200 loss: 0.10099196434020996 208.9514923095703\n",
      "28 300 loss: 0.17894522845745087 225.4176025390625\n",
      "28 400 loss: 0.15623845160007477 211.339111328125\n",
      "28 test acc: 0.8919\n",
      "29 0 loss: 0.06987877935171127 252.81365966796875\n",
      "29 100 loss: 0.1447354555130005 184.284912109375\n",
      "29 200 loss: 0.1923973262310028 200.75668334960938\n",
      "29 300 loss: 0.12977105379104614 281.73388671875\n",
      "29 400 loss: 0.09518977999687195 265.4681701660156\n",
      "29 test acc: 0.8916\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "\n",
    "\n",
    "    for step, (x,y) in enumerate(db):\n",
    "\n",
    "        # x: [b, 28, 28] => [b, 784]\n",
    "        # y: [b]\n",
    "        x = tf.reshape(x, [-1, 28*28])\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # [b, 784] => [b, 10]\n",
    "            logits = model(x)\n",
    "            y_onehot = tf.one_hot(y, depth=10)\n",
    "            # [b]\n",
    "            loss_mse = tf.reduce_mean(tf.losses.MSE(y_onehot, logits))\n",
    "            loss_ce = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)\n",
    "            loss_ce = tf.reduce_mean(loss_ce)\n",
    "\n",
    "        grads = tape.gradient(loss_ce, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(epoch, step, 'loss:', float(loss_ce), float(loss_mse))\n",
    "\n",
    "\n",
    "    # test\n",
    "    total_correct = 0\n",
    "    total_num = 0\n",
    "    for x,y in db_test:\n",
    "\n",
    "        # x: [b, 28, 28] => [b, 784]\n",
    "        # y: [b]\n",
    "        x = tf.reshape(x, [-1, 28*28])\n",
    "        # [b, 10]\n",
    "        logits = model(x)\n",
    "        # logits => prob, [b, 10]\n",
    "        prob = tf.nn.softmax(logits, axis=1)\n",
    "        # [b, 10] => [b], int64\n",
    "        pred = tf.argmax(prob, axis=1)\n",
    "        pred = tf.cast(pred, dtype=tf.int32)\n",
    "        # pred:[b]\n",
    "        # y: [b]\n",
    "        # correct: [b], True: equal, False: not equal\n",
    "        correct = tf.equal(pred, y)\n",
    "        correct = tf.reduce_sum(tf.cast(correct, dtype=tf.int32))\n",
    "\n",
    "        total_correct += int(correct)\n",
    "        total_num += x.shape[0]\n",
    "\n",
    "    acc = total_correct / total_num\n",
    "    print(epoch, 'test acc:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cifar10/100\n",
    "cifar10 和 cifar100是相同的数据集  只是标签不同，10是大类， 100是小类\n",
    "\n",
    "![cifar10](images/cifar10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cifar10/100\n",
    "(x, y), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "x.shape\n",
    "y.shape\n",
    "x_test.shape\n",
    "y_test.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.Dataset\n",
    "db = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "next(iter(db))[0].shape\n",
    "\n",
    "# .shuffle\n",
    "db = db.shuffle(10000)\n",
    "\n",
    "# .map\n",
    "def preprocess(x, y):\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    y = tf.one_hot(y, depth=10)\n",
    "    return x, y\n",
    "\n",
    "db2 = db.map(preprocess)\n",
    "res = next(iter(db2))\n",
    "res[0].shape, res[1].shape\n",
    "\n",
    "# .batch\n",
    "db3 = db2.batch(32)\n",
    "res=next(iter(db3))\n",
    "res[0].shape, res[1].shape\n",
    "\n",
    "# StopIteration\n",
    "# 通过try catch不停的迭代\n",
    "# db_iter = iter(db3)\n",
    "# while True:\n",
    "#     next(db_iter)\n",
    "\n",
    "\n",
    "# .repeat\n",
    "db4 = db3.repeat()# 数据无限循环\n",
    "db4 = db3.repeat(2)#  数据重复n次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "def prepare_minst_features_and_labels(X, y):\n",
    "    X = tf.cast(X, tf.float32) / 255.\n",
    "    y = tf.cast(y, tf.int64)\n",
    "    return X, y\n",
    "\n",
    "def mnist_dataset():\n",
    "    (x, y), (x_val, y_val) = keras.datasets.fashion_mnist.load_data()\n",
    "    y = tf.one_hot(y, depth=10)\n",
    "    y_val = tf.one_hot(y_val, depth=10)\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    ds = ds.map(prepare_minst_features_and_labels)\n",
    "    ds = ds.shuffle(60000).batch(100)\n",
    "    ds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "    ds_val = ds_val.map(prepare_minst_features_and_labels)\n",
    "    ds_val = ds_val.shuffle(10000).batch(100)\n",
    "    return ds, ds_val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bafc338abc57834bacf3c306014ce0c45f0aeaf201b721c6f2bfe17ccf7009fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
