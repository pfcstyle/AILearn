{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape:(60000, 28, 28), y shape:(60000,)\n",
      "batch: (128, 28, 28) (128,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, optimizers, Sequential, metrics\n",
    "from matplotlib import pyplot as plt\n",
    "import  io\n",
    "\n",
    "(x, y), (x_test, y_test) = keras.datasets.fashion_mnist.load_data() # 返回numpy\n",
    "print(f\"x shape:{x.shape}, y shape:{y.shape}\")\n",
    "\n",
    "def preprocess(x, y):\n",
    "\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x,y\n",
    "\n",
    "batchsz = 128\n",
    "db = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "db = db.map(preprocess).shuffle(10000).batch(batchsz)\n",
    "db_iter = iter(db)\n",
    "sample = next(db_iter)\n",
    "print('batch:', sample[0].shape, sample[1].shape)\n",
    "\n",
    "db_test = tf.data.Dataset.from_tensor_slices((x_test,y_test))\n",
    "db_test = db_test.map(preprocess).batch(batchsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               200960    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 244,522\n",
      "Trainable params: 244,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Dense(256, activation=tf.nn.relu), # [b, 784] => [b, 256]\n",
    "    layers.Dense(128, activation=tf.nn.relu), # [b, 256] => [b, 128]\n",
    "    layers.Dense(64, activation=tf.nn.relu), # [b, 128] => [b, 64]\n",
    "    layers.Dense(32, activation=tf.nn.relu), # [b, 64] => [b, 32]\n",
    "    layers.Dense(10) # [b, 32] => [b, 10], 330 = 32*10 + 10\n",
    "])\n",
    "model.build(input_shape=[None, 28*28])\n",
    "model.summary()\n",
    "# w = w - lr*grad\n",
    "optimizer = optimizers.Adam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 loss: 2.328538179397583 0.18116885423660278\n",
      "0 100 loss: 0.5295809507369995 35.6922721862793\n",
      "0 200 loss: 0.5176625847816467 28.342437744140625\n",
      "0 300 loss: 0.5652111172676086 40.81658172607422\n",
      "0 400 loss: 0.42393407225608826 34.39350509643555\n",
      "0 test acc: 0.8452\n",
      "1 0 loss: 0.5498256087303162 30.525524139404297\n",
      "1 100 loss: 0.25784212350845337 34.683197021484375\n",
      "1 200 loss: 0.2835013270378113 32.9970817565918\n",
      "1 300 loss: 0.5449256896972656 42.74700927734375\n",
      "1 400 loss: 0.3391828238964081 39.91959762573242\n",
      "1 test acc: 0.8587\n",
      "2 0 loss: 0.3791736960411072 36.629451751708984\n",
      "2 100 loss: 0.2740439474582672 37.08982849121094\n",
      "2 200 loss: 0.4671878516674042 35.50667190551758\n",
      "2 300 loss: 0.27072280645370483 40.191925048828125\n",
      "2 400 loss: 0.3873215317726135 43.14110565185547\n",
      "2 test acc: 0.8661\n",
      "3 0 loss: 0.2713596522808075 41.80927276611328\n",
      "3 100 loss: 0.30535274744033813 41.8309326171875\n",
      "3 200 loss: 0.25934654474258423 47.52680206298828\n",
      "3 300 loss: 0.2546522617340088 42.33328628540039\n",
      "3 400 loss: 0.24451300501823425 42.72760009765625\n",
      "3 test acc: 0.8768\n",
      "4 0 loss: 0.22665733098983765 43.98948287963867\n",
      "4 100 loss: 0.307472825050354 45.45389938354492\n",
      "4 200 loss: 0.2230508178472519 53.366180419921875\n",
      "4 300 loss: 0.22953814268112183 52.614540100097656\n",
      "4 400 loss: 0.3164396286010742 54.100914001464844\n",
      "4 test acc: 0.8728\n",
      "5 0 loss: 0.2772950232028961 47.46540069580078\n",
      "5 100 loss: 0.21390174329280853 63.44253158569336\n",
      "5 200 loss: 0.29457545280456543 60.118961334228516\n",
      "5 300 loss: 0.275301992893219 59.69724655151367\n",
      "5 400 loss: 0.22935593128204346 53.37052536010742\n",
      "5 test acc: 0.8757\n",
      "6 0 loss: 0.3998778164386749 50.005401611328125\n",
      "6 100 loss: 0.22319526970386505 58.250648498535156\n",
      "6 200 loss: 0.3438698947429657 53.34550476074219\n",
      "6 300 loss: 0.31408530473709106 61.04007339477539\n",
      "6 400 loss: 0.22669315338134766 81.64181518554688\n",
      "6 test acc: 0.8833\n",
      "7 0 loss: 0.21912148594856262 53.94774627685547\n",
      "7 100 loss: 0.34549635648727417 66.32594299316406\n",
      "7 200 loss: 0.26404476165771484 60.36256408691406\n",
      "7 300 loss: 0.3093280792236328 76.42810821533203\n",
      "7 400 loss: 0.1832294464111328 72.3067855834961\n",
      "7 test acc: 0.8793\n",
      "8 0 loss: 0.23308667540550232 66.95803833007812\n",
      "8 100 loss: 0.16817530989646912 72.5538330078125\n",
      "8 200 loss: 0.17554137110710144 66.62861633300781\n",
      "8 300 loss: 0.23027780652046204 61.09716796875\n",
      "8 400 loss: 0.18301817774772644 70.08146667480469\n",
      "8 test acc: 0.8862\n",
      "9 0 loss: 0.2918851375579834 82.3025894165039\n",
      "9 100 loss: 0.31554365158081055 69.35932922363281\n",
      "9 200 loss: 0.15707392990589142 84.35125732421875\n",
      "9 300 loss: 0.23452496528625488 114.90658569335938\n",
      "9 400 loss: 0.2209995836019516 83.73736572265625\n",
      "9 test acc: 0.8818\n",
      "10 0 loss: 0.21249094605445862 83.62222290039062\n",
      "10 100 loss: 0.23826977610588074 82.06061553955078\n",
      "10 200 loss: 0.1860238015651703 74.27310180664062\n",
      "10 300 loss: 0.2187209576368332 73.97887420654297\n",
      "10 400 loss: 0.2691057324409485 78.78179931640625\n",
      "10 test acc: 0.8842\n",
      "11 0 loss: 0.26859715580940247 83.09622192382812\n",
      "11 100 loss: 0.1955924928188324 87.93211364746094\n",
      "11 200 loss: 0.19335594773292542 83.70829010009766\n",
      "11 300 loss: 0.2446422576904297 75.13905334472656\n",
      "11 400 loss: 0.1798611283302307 84.85111999511719\n",
      "11 test acc: 0.8877\n",
      "12 0 loss: 0.20488597452640533 100.27601623535156\n",
      "12 100 loss: 0.12318234145641327 112.70838928222656\n",
      "12 200 loss: 0.1728372871875763 70.90599060058594\n",
      "12 300 loss: 0.15598976612091064 113.82797241210938\n",
      "12 400 loss: 0.20932236313819885 107.52539825439453\n",
      "12 test acc: 0.8916\n",
      "13 0 loss: 0.22653186321258545 86.77783966064453\n",
      "13 100 loss: 0.31059184670448303 103.674072265625\n",
      "13 200 loss: 0.2694110572338104 102.36131286621094\n",
      "13 300 loss: 0.3163191080093384 95.3759765625\n",
      "13 400 loss: 0.24518980085849762 92.247802734375\n",
      "13 test acc: 0.8854\n",
      "14 0 loss: 0.2574433386325836 92.9159927368164\n",
      "14 100 loss: 0.2684507966041565 88.28007507324219\n",
      "14 200 loss: 0.1805846095085144 110.15547180175781\n",
      "14 300 loss: 0.1515188217163086 120.76516723632812\n",
      "14 400 loss: 0.2643481492996216 95.66629028320312\n",
      "14 test acc: 0.8869\n",
      "15 0 loss: 0.18730956315994263 93.25418090820312\n",
      "15 100 loss: 0.16540288925170898 112.66831970214844\n",
      "15 200 loss: 0.1787533313035965 103.18936920166016\n",
      "15 300 loss: 0.2558400630950928 89.71429443359375\n",
      "15 400 loss: 0.2550797760486603 107.80239868164062\n",
      "15 test acc: 0.8848\n",
      "16 0 loss: 0.18263502418994904 127.819091796875\n",
      "16 100 loss: 0.17803940176963806 94.15774536132812\n",
      "16 200 loss: 0.2672324776649475 104.61114501953125\n",
      "16 300 loss: 0.2426365464925766 147.64297485351562\n",
      "16 400 loss: 0.12587489187717438 129.70689392089844\n",
      "16 test acc: 0.8878\n",
      "17 0 loss: 0.14656829833984375 128.34353637695312\n",
      "17 100 loss: 0.1512494534254074 130.9329071044922\n",
      "17 200 loss: 0.1854488104581833 124.49501037597656\n",
      "17 300 loss: 0.10775411874055862 171.56085205078125\n",
      "17 400 loss: 0.20360055565834045 158.9936981201172\n",
      "17 test acc: 0.8885\n",
      "18 0 loss: 0.17633751034736633 128.72491455078125\n",
      "18 100 loss: 0.1201215460896492 155.6826171875\n",
      "18 200 loss: 0.15450698137283325 143.4352264404297\n",
      "18 300 loss: 0.19586199522018433 155.91358947753906\n",
      "18 400 loss: 0.11513465642929077 124.60328674316406\n",
      "18 test acc: 0.8896\n",
      "19 0 loss: 0.12730225920677185 143.772216796875\n",
      "19 100 loss: 0.24025242030620575 179.125244140625\n",
      "19 200 loss: 0.1644527018070221 131.50001525878906\n",
      "19 300 loss: 0.1589518040418625 176.10903930664062\n",
      "19 400 loss: 0.14029164612293243 194.15545654296875\n",
      "19 test acc: 0.8884\n",
      "20 0 loss: 0.23856396973133087 130.16415405273438\n",
      "20 100 loss: 0.1482677310705185 169.06668090820312\n",
      "20 200 loss: 0.10209918022155762 153.96737670898438\n",
      "20 300 loss: 0.15675368905067444 168.24649047851562\n",
      "20 400 loss: 0.19093385338783264 226.36766052246094\n",
      "20 test acc: 0.8929\n",
      "21 0 loss: 0.25940948724746704 161.765869140625\n",
      "21 100 loss: 0.1355528086423874 165.2210693359375\n",
      "21 200 loss: 0.1326911747455597 210.48684692382812\n",
      "21 300 loss: 0.06654722988605499 214.18727111816406\n",
      "21 400 loss: 0.18682892620563507 198.10594177246094\n",
      "21 test acc: 0.8887\n",
      "22 0 loss: 0.13792940974235535 147.19619750976562\n",
      "22 100 loss: 0.14576148986816406 227.73641967773438\n",
      "22 200 loss: 0.1416742205619812 183.26095581054688\n",
      "22 300 loss: 0.09917522966861725 172.4638671875\n",
      "22 400 loss: 0.22441977262496948 177.4144287109375\n",
      "22 test acc: 0.8909\n",
      "23 0 loss: 0.1810816079378128 167.97962951660156\n",
      "23 100 loss: 0.1495257318019867 175.29055786132812\n",
      "23 200 loss: 0.1465790569782257 170.30844116210938\n",
      "23 300 loss: 0.20478078722953796 195.052978515625\n",
      "23 400 loss: 0.1099298894405365 220.39663696289062\n",
      "23 test acc: 0.8897\n",
      "24 0 loss: 0.17793872952461243 190.73524475097656\n",
      "24 100 loss: 0.09694838523864746 257.00885009765625\n",
      "24 200 loss: 0.16318778693675995 235.4788818359375\n",
      "24 300 loss: 0.11239214241504669 202.4690399169922\n",
      "24 400 loss: 0.16312570869922638 206.24502563476562\n",
      "24 test acc: 0.8878\n",
      "25 0 loss: 0.22796529531478882 261.1403503417969\n",
      "25 100 loss: 0.10700833052396774 222.56723022460938\n",
      "25 200 loss: 0.15270504355430603 259.55914306640625\n",
      "25 300 loss: 0.1525726169347763 219.52645874023438\n",
      "25 400 loss: 0.14234879612922668 197.75523376464844\n",
      "25 test acc: 0.8961\n",
      "26 0 loss: 0.1283407062292099 195.6072235107422\n",
      "26 100 loss: 0.1674577295780182 220.2087860107422\n",
      "26 200 loss: 0.11069333553314209 255.11338806152344\n",
      "26 300 loss: 0.16084527969360352 301.77349853515625\n",
      "26 400 loss: 0.11770546436309814 283.71160888671875\n",
      "26 test acc: 0.8915\n",
      "27 0 loss: 0.15483491122722626 262.16668701171875\n",
      "27 100 loss: 0.13665370643138885 256.7565002441406\n",
      "27 200 loss: 0.19224923849105835 257.0796813964844\n",
      "27 300 loss: 0.11158815026283264 205.05322265625\n",
      "27 400 loss: 0.1803026646375656 244.81600952148438\n",
      "27 test acc: 0.891\n",
      "28 0 loss: 0.11698941886425018 265.654296875\n",
      "28 100 loss: 0.12583112716674805 277.2486267089844\n",
      "28 200 loss: 0.1649351716041565 254.44821166992188\n",
      "28 300 loss: 0.16341161727905273 290.2694091796875\n",
      "28 400 loss: 0.14364515244960785 315.5289306640625\n",
      "28 test acc: 0.8898\n",
      "29 0 loss: 0.11268024146556854 336.7711181640625\n",
      "29 100 loss: 0.0644187405705452 380.6907958984375\n",
      "29 200 loss: 0.09208264946937561 296.43560791015625\n",
      "29 300 loss: 0.06495043635368347 381.0096740722656\n",
      "29 400 loss: 0.15577402710914612 389.6156005859375\n",
      "29 test acc: 0.8932\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "\n",
    "\n",
    "    for step, (x,y) in enumerate(db):\n",
    "\n",
    "        # x: [b, 28, 28] => [b, 784]\n",
    "        # y: [b]\n",
    "        x = tf.reshape(x, [-1, 28*28])\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # [b, 784] => [b, 10]\n",
    "            logits = model(x)\n",
    "            y_onehot = tf.one_hot(y, depth=10)\n",
    "            # [b]\n",
    "            loss_mse = tf.reduce_mean(tf.losses.MSE(y_onehot, logits))\n",
    "            loss_ce = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)\n",
    "            loss_ce = tf.reduce_mean(loss_ce)\n",
    "\n",
    "        grads = tape.gradient(loss_ce, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(epoch, step, 'loss:', float(loss_ce), float(loss_mse))\n",
    "\n",
    "\n",
    "    # test\n",
    "    total_correct = 0\n",
    "    total_num = 0\n",
    "    for x,y in db_test:\n",
    "\n",
    "        # x: [b, 28, 28] => [b, 784]\n",
    "        # y: [b]\n",
    "        x = tf.reshape(x, [-1, 28*28])\n",
    "        # [b, 10]\n",
    "        logits = model(x)\n",
    "        # logits => prob, [b, 10]\n",
    "        prob = tf.nn.softmax(logits, axis=1)\n",
    "        # [b, 10] => [b], int64\n",
    "        pred = tf.argmax(prob, axis=1)\n",
    "        pred = tf.cast(pred, dtype=tf.int32)\n",
    "        # pred:[b]\n",
    "        # y: [b]\n",
    "        # correct: [b], True: equal, False: not equal\n",
    "        correct = tf.equal(pred, y)\n",
    "        correct = tf.reduce_sum(tf.cast(correct, dtype=tf.int32))\n",
    "\n",
    "        total_correct += int(correct)\n",
    "        total_num += x.shape[0]\n",
    "\n",
    "    acc = total_correct / total_num\n",
    "    print(epoch, 'test acc:', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('new_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d58ff1fa528449bec86ac3b87e5b9edec9a0f46f3f9c706338d5512923abfa3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
