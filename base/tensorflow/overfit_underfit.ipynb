{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, optimizers, Sequential, metrics, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型的度量\n",
    "\n",
    "ground-truth: 人们已经大概知道某数据应该符合什么分布\n",
    "\n",
    "## 1.model capacity\n",
    "模型次方越多， 表达能力越强\n",
    "\n",
    "模型的层数越多，表达能力越强\n",
    "\n",
    "## underfit\n",
    "训练和验证的精确度都很低，不需要特殊检测\n",
    "\n",
    "## overfit\n",
    "使用交叉验证,交叉验证的原理是不断的切换从训练集挑选验证集的位置，切换的时机可以是epoch等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets: (60000, 28, 28) (60000,) 0 255\n"
     ]
    }
   ],
   "source": [
    "# 交叉验证\n",
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    x is a simple image, not a batch\n",
    "    \"\"\"\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
    "    x = tf.reshape(x, [28*28])\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    y = tf.one_hot(y, depth=10)\n",
    "    return x,y\n",
    "\n",
    "\n",
    "batchsz = 128\n",
    "(x, y), (x_test, y_test) = datasets.mnist.load_data()\n",
    "print('datasets:', x.shape, y.shape, x.min(), x.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "db_test = db_test.map(preprocess).batch(batchsz) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, 256)               200960    \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 244,522\n",
      "Trainable params: 244,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network = Sequential([layers.Dense(256, activation='relu'),\n",
    "                     layers.Dense(128, activation='relu'),\n",
    "                     layers.Dense(64, activation='relu'),\n",
    "                     layers.Dense(32, activation='relu'),\n",
    "                     layers.Dense(10)])\n",
    "network.build(input_shape=(None, 28*28))\n",
    "network.summary()\n",
    "\n",
    "network.compile(optimizer=optimizers.Adam(learning_rate=0.01),\n",
    "\t\tloss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
    "\t\tmetrics=['accuracy']\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28) (50000,) (10000, 28, 28) (10000,)\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0671 - accuracy: 0.9829 - val_loss: 0.0496 - val_accuracy: 0.9878\n",
      "(50000, 28, 28) (50000,) (10000, 28, 28) (10000,)\n",
      "391/391 [==============================] - 3s 5ms/step - loss: 0.0638 - accuracy: 0.9839 - val_loss: 0.0490 - val_accuracy: 0.9863\n",
      "(50000, 28, 28) (50000,) (10000, 28, 28) (10000,)\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0619 - accuracy: 0.9852 - val_loss: 0.0858 - val_accuracy: 0.9793\n",
      "(50000, 28, 28) (50000,) (10000, 28, 28) (10000,)\n",
      "391/391 [==============================] - 3s 5ms/step - loss: 0.0722 - accuracy: 0.9832 - val_loss: 0.0755 - val_accuracy: 0.9820\n",
      "(50000, 28, 28) (50000,) (10000, 28, 28) (10000,)\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0565 - accuracy: 0.9863 - val_loss: 0.0528 - val_accuracy: 0.9860\n",
      "Test performance:\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.1125 - accuracy: 0.9765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11246237903833389, 0.9764999747276306]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    idx = tf.range(60000)\n",
    "    idx = tf.random.shuffle(idx) # 每个epoch打乱数据索引\n",
    "    # x_train, x_val = tf.split(x, num_or_size_splits=[50000, 10000])\n",
    "    # y_train, y_val = tf.split(y, num_or_size_splits=[50000, 10000])\n",
    "    x_train, y_train = tf.gather(x, idx[:50000]), tf.gather(y, idx[:50000])\n",
    "    x_val, y_val = tf.gather(x, idx[-10000:]) , tf.gather(y, idx[-10000:])\n",
    "    print(x_train.shape, y_train.shape, x_val.shape, y_val.shape)\n",
    "    db_train = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "    db_train = db_train.map(preprocess).shuffle(50000).batch(batchsz)\n",
    "\n",
    "    db_val = tf.data.Dataset.from_tensor_slices((x_val,y_val))\n",
    "    db_val = db_val.map(preprocess).shuffle(10000).batch(batchsz)\n",
    "\n",
    "    network.fit(db_train, validation_data=db_val, validation_freq=1)\n",
    "\n",
    "print('Test performance:') \n",
    "network.evaluate(db_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (60000, 784) y shape: (60000, 10)\n",
      "Epoch 1/6\n",
      "399/399 [==============================] - 2s 4ms/step - loss: 0.0690 - accuracy: 0.9852\n",
      "Epoch 2/6\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0440 - accuracy: 0.9903 - val_loss: 0.0803 - val_accuracy: 0.9843\n",
      "Epoch 3/6\n",
      "399/399 [==============================] - 2s 4ms/step - loss: 0.0624 - accuracy: 0.9866\n",
      "Epoch 4/6\n",
      "399/399 [==============================] - 2s 4ms/step - loss: 0.0397 - accuracy: 0.9906 - val_loss: 0.0736 - val_accuracy: 0.9854\n",
      "Epoch 5/6\n",
      "399/399 [==============================] - 2s 4ms/step - loss: 0.0394 - accuracy: 0.9907\n",
      "Epoch 6/6\n",
      "399/399 [==============================] - 2s 4ms/step - loss: 0.0491 - accuracy: 0.9895 - val_loss: 0.0955 - val_accuracy: 0.9820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x233b6233130>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用tensorflow自动随机划分验证集\n",
    "x_train_val = tf.convert_to_tensor(x)\n",
    "x_train_val = tf.cast(x_train_val, dtype=tf.float32) / 255\n",
    "x_train_val = tf.reshape(x_train_val, (60000, 28 * 28))\n",
    "y_train_val = tf.cast(y, dtype=tf.int32)\n",
    "y_train_val = tf.one_hot(y_train_val, depth=10)\n",
    "print(\"x shape:\", x_train_val.shape, \"y shape:\", y_train_val.shape)\n",
    "# db_train_val = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "# db_train_val = db_train_val.map(preprocess).shuffle(60000).batch(batchsz)\n",
    "# validation_split不支持dataset，只能使用tensor或者numpy\n",
    "network.fit(x_train_val, y_train_val, epochs=6, batch_size=batchsz, validation_split=0.15, validation_freq=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解决overfit\n",
    "1. 使用更多的数据\n",
    "2. 约束模型复杂度\n",
    "   1. 根据数据集，选择合适的网络结构\n",
    "   2. 可以先从大（深）的网络开始，不停的降低复杂度（regularization)\n",
    "3. 动量与学习率，动量是上次梯度和本次梯度的矢量和\n",
    "   \n",
    "   ![Momentum](./images/Momentum.png)\n",
    "   \n",
    "4. 删除一些参数(Dropout), 不再全连接，层与层之间稀疏连接\n",
    "5. Early Stopping, 提早结束训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization\n",
    "核心原理就是让高维参数接近0，降低模型复杂度，从而解决overfit\n",
    "\n",
    "![regularization](./images/regularization.png)\n",
    "\n",
    "缺点：限制模型的表达能力，下图上面三个图是使用了Regularization，下面三个没用。\n",
    "\n",
    "![regularization_short](./images/regularization_short.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization(Weigth decay) 手动实现\n",
    "# 在计算loss时，为模型变量增加l2范数\n",
    "\n",
    "def preprocess(x, y):\n",
    "\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "\n",
    "    return x,y\n",
    "\n",
    "\n",
    "batchsz = 128\n",
    "(x, y), (x_val, y_val) = datasets.mnist.load_data()\n",
    "print('datasets:', x.shape, y.shape, x.min(), x.max())\n",
    "\n",
    "\n",
    "\n",
    "db = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "db = db.map(preprocess).shuffle(60000).batch(batchsz).repeat(10)\n",
    "\n",
    "ds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "ds_val = ds_val.map(preprocess).batch(batchsz) \n",
    "\n",
    "\n",
    "network = Sequential([layers.Dense(256, activation='relu'),\n",
    "                     layers.Dense(128, activation='relu'),\n",
    "                     layers.Dense(64, activation='relu'),\n",
    "                     layers.Dense(32, activation='relu'),\n",
    "                     layers.Dense(10)])\n",
    "network.build(input_shape=(None, 28*28))\n",
    "network.summary()\n",
    "\n",
    "optimizer = optimizers.Adam(lr=0.01)\n",
    "\n",
    "for step, (x,y) in enumerate(db):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # [b, 28, 28] => [b, 784]\n",
    "        x = tf.reshape(x, (-1, 28*28))\n",
    "        # [b, 784] => [b, 10]\n",
    "        out = network(x)\n",
    "        # [b] => [b, 10]\n",
    "        y_onehot = tf.one_hot(y, depth=10) \n",
    "        # [b]\n",
    "        loss = tf.reduce_mean(tf.losses.categorical_crossentropy(y_onehot, out, from_logits=True))\n",
    "\n",
    "\n",
    "        loss_regularization = []\n",
    "        for p in network.trainable_variables:\n",
    "            loss_regularization.append(tf.nn.l2_loss(p))\n",
    "        # 这里求和必须是个tensor，所以需要stack合并一下\n",
    "        loss_regularization = tf.reduce_sum(tf.stack(loss_regularization))\n",
    "\n",
    "        loss = loss + 0.0001 * loss_regularization\n",
    " \n",
    "\n",
    "    grads = tape.gradient(loss, network.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
    "\n",
    "\n",
    "    if step % 100 == 0:\n",
    "\n",
    "        print(step, 'loss:', float(loss), 'loss_regularization:', float(loss_regularization)) \n",
    "\n",
    "\n",
    "    # evaluate\n",
    "    if step % 500 == 0:\n",
    "        total, total_correct = 0., 0\n",
    "\n",
    "        for step, (x, y) in enumerate(ds_val): \n",
    "            # [b, 28, 28] => [b, 784]\n",
    "            x = tf.reshape(x, (-1, 28*28))\n",
    "            # [b, 784] => [b, 10]\n",
    "            out = network(x) \n",
    "            # [b, 10] => [b] \n",
    "            pred = tf.argmax(out, axis=1) \n",
    "            pred = tf.cast(pred, dtype=tf.int32)\n",
    "            # bool type \n",
    "            correct = tf.equal(pred, y)\n",
    "            # bool tensor => int tensor => numpy\n",
    "            total_correct += tf.reduce_sum(tf.cast(correct, dtype=tf.int32)).numpy()\n",
    "            total += x.shape[0]\n",
    "\n",
    "        print(step, 'Evaluate Acc:', total_correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用框架\n",
    "network = Sequential([layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "                     layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "                     layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "                     layers.Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "                     layers.Dense(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momentum\n",
    "# 一般是设在optimizer中\n",
    "# 动量比率一般设为0.9\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.02, momentum=0.9)\n",
    "optimizer = tf.optimizers.RMSprop(learning_rate=0.02, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic learning rate\n",
    "# 这只是一个示例  可以任意调整\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.2)\n",
    "for epoch in range(100):\n",
    "    optimizer.learning_rate = 0.2 * (100 - epoch) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout  可以和regularization同用\n",
    "network = Sequential([layers.Dense(256, activation='relu'),\n",
    "                     layers.Dropout(0.5), # 0.5 rate to drop 注意在训练和测试时，需要在network标记，看下面代码\n",
    "                     layers.Dense(128, activation='relu'),\n",
    "                     layers.Dropout(0.5), # 0.5 rate to drop\n",
    "                     layers.Dense(64, activation='relu'),\n",
    "                     layers.Dense(32, activation='relu'),\n",
    "                     layers.Dense(10)])\n",
    "\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "\n",
    "\n",
    "for step, (x,y) in enumerate(db):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # [b, 28, 28] => [b, 784]\n",
    "        x = tf.reshape(x, (-1, 28*28))\n",
    "        # [b, 784] => [b, 10]\n",
    "        # 标记这是训练过程\n",
    "        out = network(x, training=True)\n",
    "        # [b] => [b, 10]\n",
    "        y_onehot = tf.one_hot(y, depth=10) \n",
    "        # [b]\n",
    "        loss = tf.reduce_mean(tf.losses.categorical_crossentropy(y_onehot, out, from_logits=True))\n",
    "\n",
    "\n",
    "        loss_regularization = []\n",
    "        for p in network.trainable_variables:\n",
    "            loss_regularization.append(tf.nn.l2_loss(p))\n",
    "        loss_regularization = tf.reduce_sum(tf.stack(loss_regularization))\n",
    "\n",
    "        loss = loss + 0.0001 * loss_regularization\n",
    " \n",
    "\n",
    "    grads = tape.gradient(loss, network.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
    "\n",
    "\n",
    "    if step % 100 == 0:\n",
    "\n",
    "        print(step, 'loss:', float(loss), 'loss_regularization:', float(loss_regularization)) \n",
    "\n",
    "\n",
    "    # evaluate\n",
    "    if step % 500 == 0:\n",
    "        total, total_correct = 0., 0\n",
    "\n",
    "        for step, (x, y) in enumerate(ds_val): \n",
    "            # [b, 28, 28] => [b, 784]\n",
    "            x = tf.reshape(x, (-1, 28*28))\n",
    "            # [b, 784] => [b, 10] \n",
    "            out = network(x, training=False)  \n",
    "            # [b, 10] => [b] \n",
    "            pred = tf.argmax(out, axis=1) \n",
    "            pred = tf.cast(pred, dtype=tf.int32)\n",
    "            # bool type \n",
    "            correct = tf.equal(pred, y)\n",
    "            # bool tensor => int tensor => numpy\n",
    "            total_correct += tf.reduce_sum(tf.cast(correct, dtype=tf.int32)).numpy()\n",
    "            total += x.shape[0]\n",
    "\n",
    "        print(step, 'Evaluate Acc with drop:', total_correct/total)\n",
    "\n",
    "        total, total_correct = 0., 0\n",
    "\n",
    "        for step, (x, y) in enumerate(ds_val): \n",
    "            # [b, 28, 28] => [b, 784]\n",
    "            x = tf.reshape(x, (-1, 28*28))\n",
    "            # [b, 784] => [b, 10] \n",
    "            # 标记非训练过程\n",
    "            out = network(x, training=False)  \n",
    "            # [b, 10] => [b] \n",
    "            pred = tf.argmax(out, axis=1) \n",
    "            pred = tf.cast(pred, dtype=tf.int32)\n",
    "            # bool type \n",
    "            correct = tf.equal(pred, y)\n",
    "            # bool tensor => int tensor => numpy\n",
    "            total_correct += tf.reduce_sum(tf.cast(correct, dtype=tf.int32)).numpy()\n",
    "            total += x.shape[0]\n",
    "\n",
    "        print(step, 'Evaluate Acc without drop:', total_correct/total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('new_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d58ff1fa528449bec86ac3b87e5b9edec9a0f46f3f9c706338d5512923abfa3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
