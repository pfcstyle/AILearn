{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) <dtype: 'float32'> <dtype: 'int32'>\n",
      "tf.Tensor(0.0, shape=(), dtype=float32) tf.Tensor(255.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0, shape=(), dtype=int32) tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# x: [60k, 28, 28]\n",
    "# y: [60k]\n",
    "(x, y), (x_test, y_test) = datasets.mnist.load_data()\n",
    "\n",
    "x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "y = tf.convert_to_tensor(y, dtype=tf.int32)\n",
    "x_test = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
    "y_test = tf.convert_to_tensor(y_test, dtype=tf.int32)\n",
    "\n",
    "print(x.shape, y.shape, x.dtype, y.dtype)\n",
    "print(tf.reduce_min(x), tf.reduce_max(x))\n",
    "print(tf.reduce_min(y), tf.reduce_max(y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: (128, 28, 28) (128,)\n"
     ]
    }
   ],
   "source": [
    "# 创建数据集，方便取batch\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x, y)).batch(128)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(128)\n",
    "train_iter = iter(train_db) # 获取迭代器\n",
    "sample = next(train_iter)\n",
    "print('batch:', sample[0].shape, sample[1].shape) # 从输出可以看到  每次读取了128张图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 loss: 0.12915447354316711\n",
      "0 100 loss: 0.02183777466416359\n",
      "0 200 loss: 0.014754688367247581\n",
      "0 300 loss: 0.014491741545498371\n",
      "0 400 loss: 0.01534547470510006\n",
      "test acc: 0.938\n",
      "1 0 loss: 0.011050170287489891\n",
      "1 100 loss: 0.01046844944357872\n",
      "1 200 loss: 0.012057008221745491\n",
      "1 300 loss: 0.009639924392104149\n",
      "1 400 loss: 0.011321398429572582\n",
      "test acc: 0.9566\n",
      "2 0 loss: 0.008592674508690834\n",
      "2 100 loss: 0.008449897170066833\n",
      "2 200 loss: 0.010562802664935589\n",
      "2 300 loss: 0.007304591126739979\n",
      "2 400 loss: 0.009837128221988678\n",
      "test acc: 0.9652\n",
      "3 0 loss: 0.007479909807443619\n",
      "3 100 loss: 0.007153123617172241\n",
      "3 200 loss: 0.009035671129822731\n",
      "3 300 loss: 0.006315006408840418\n",
      "3 400 loss: 0.008303827606141567\n",
      "test acc: 0.9679\n",
      "4 0 loss: 0.006521566770970821\n",
      "4 100 loss: 0.006263895891606808\n",
      "4 200 loss: 0.007643335964530706\n",
      "4 300 loss: 0.0056782858446240425\n",
      "4 400 loss: 0.007644444704055786\n",
      "test acc: 0.9712\n",
      "5 0 loss: 0.005835401825606823\n",
      "5 100 loss: 0.005733797326683998\n",
      "5 200 loss: 0.006747194565832615\n",
      "5 300 loss: 0.005204974208027124\n",
      "5 400 loss: 0.007268063724040985\n",
      "test acc: 0.9728\n",
      "6 0 loss: 0.00520953768864274\n",
      "6 100 loss: 0.004969969391822815\n",
      "6 200 loss: 0.006018585525453091\n",
      "6 300 loss: 0.004808842670172453\n",
      "6 400 loss: 0.006749191787093878\n",
      "test acc: 0.9741\n",
      "7 0 loss: 0.004814581014215946\n",
      "7 100 loss: 0.004328259266912937\n",
      "7 200 loss: 0.005435711704194546\n",
      "7 300 loss: 0.0044988347217440605\n",
      "7 400 loss: 0.006445036269724369\n",
      "test acc: 0.9744\n",
      "8 0 loss: 0.004918135702610016\n",
      "8 100 loss: 0.00399128207936883\n",
      "8 200 loss: 0.005024694371968508\n",
      "8 300 loss: 0.004273028112947941\n",
      "8 400 loss: 0.006143974605947733\n",
      "test acc: 0.9755\n",
      "9 0 loss: 0.004459476098418236\n",
      "9 100 loss: 0.003589573549106717\n",
      "9 200 loss: 0.004813038744032383\n",
      "9 300 loss: 0.0040637431666255\n",
      "9 400 loss: 0.005867421627044678\n",
      "test acc: 0.9766\n"
     ]
    }
   ],
   "source": [
    "# y = x@w + b\n",
    "# Input => Out: [b, 784] => [b, 256] => [b, 128] => [b, 10] \n",
    "# [dim_in, dim_out], [dim_out]\n",
    "# 初始化权重很重要，尤其注意范围，否则很可能导致梯度爆炸\n",
    "w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.01)) # 为了支持tf.GradientTape，需要转为tf.Variable\n",
    "b1 = tf.Variable(tf.zeros([256]))\n",
    "w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.01))\n",
    "b2 = tf.Variable(tf.zeros([128]))\n",
    "w3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev=0.01))\n",
    "b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "lr = 1e-1 # 学习率过大会导致gradient exploding\n",
    "\n",
    "for epoch in range(10): # iterate db for 10 数据不够，多迭代几次\n",
    "    for step, (x, y) in enumerate(train_db):\n",
    "        # x: [128, 28, 28]\n",
    "        # y: [128]\n",
    "        x = tf.reshape(x, [-1, 28 * 28]) # x: [b, 28 * 28]\n",
    "        # [b, 784]@[784, 256] + [256] => [b, 256] + [256]\n",
    "\n",
    "        with tf.GradientTape() as tape: # 记录梯度 只能记录类型为tf.Variable的变量\n",
    "            # tape.watch([w1, b1, w2, b2, w3, b3]) # 这里如果不手动watch,就需要把变量声明为tf.Variable\n",
    "\n",
    "            h1 = x@w1 + b1\n",
    "            h1 = tf.nn.relu(h1)\n",
    "            h2 = h1@w2 + b2\n",
    "            h2 = tf.nn.relu(h2)\n",
    "            out = h2@w3 + b3\n",
    "\n",
    "            # compute loss\n",
    "            # out: [b, 10]\n",
    "            # y: [b] => [b, 10]\n",
    "            y_onehot = tf.one_hot(y, depth=10) # one-hot encoding\n",
    "\n",
    "            # mse = mean(sum(y-out)^2)\n",
    "            # [b, 10]\n",
    "            loss = tf.square(y_onehot - out)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "        # compute gradients\n",
    "        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\n",
    "        grads, _ = tf.clip_by_global_norm(grads, 15) # 解决gradient exploding\n",
    "        # w1 = w1 - lr * w1_grad\n",
    "        w1.assign_sub(lr * grads[0])\n",
    "        b1.assign_sub(lr * grads[1])\n",
    "        w2.assign_sub(lr * grads[2])\n",
    "        b2.assign_sub(lr * grads[3])\n",
    "        w3.assign_sub(lr * grads[4])\n",
    "        b3.assign_sub(lr * grads[5])\n",
    "        # w1 = tf.Variable(w1 - lr * grads[0]) \n",
    "        # b1 = tf.Variable(b1 - lr * grads[1])\n",
    "        # w2 = tf.Variable(w2 - lr * grads[2])\n",
    "        # b2 = tf.Variable(b2 - lr * grads[3])\n",
    "        # w3 = tf.Variable(w3 - lr * grads[4])\n",
    "        # b3 = tf.Variable(b3 - lr * grads[5])\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(epoch, step, 'loss:', float(loss))\n",
    "\n",
    "    # test/evluation\n",
    "    # [w1, b1, w2, b2, w3, b3]\n",
    "    total_correct, total_num = 0, 0\n",
    "    for step, (x, y) in enumerate(test_db):\n",
    "        # [b, 28, 28] => [b, 28 * 28]\n",
    "        x = tf.reshape(x, [-1, 28*28])\n",
    "        # [b, 784] => [b, 256] => [b, 128] => [b, 10]\n",
    "        h1 = tf.nn.relu(x@w1 + b1)\n",
    "        h2 = tf.nn.relu(h1@w2 + b2)\n",
    "        out = h2@w3 + b3\n",
    "        # out: [b, 10] ~ R\n",
    "        # prob: [b, 10] ~ [0, 1]\n",
    "        prob = tf.nn.softmax(out, axis=1)\n",
    "        # [b, 10] => [b]\n",
    "        pred = tf.cast(tf.argmax(prob, axis=1), dtype=tf.int32)\n",
    "        # y: [b]\n",
    "        correct = tf.cast(tf.equal(pred, y), dtype=tf.int32)\n",
    "        correct = tf.reduce_sum(correct)\n",
    "        total_correct += int(correct)\n",
    "        total_num += x.shape[0]\n",
    "    acc = total_correct / total_num\n",
    "    print('test acc:', acc)\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('new_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d58ff1fa528449bec86ac3b87e5b9edec9a0f46f3f9c706338d5512923abfa3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
