{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf AutoGrad\n",
    "# 这里persistent如果为false, 每次tape.gradient()之后会自动释放资源，True则需要手动释放\n",
    "# 如果需要多次计算梯度（tape.gradient()）则需要persistent=True\n",
    "# with tf.GradientTape(persistent=True) as tape:\n",
    "# grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 链式法则\n",
    "\n",
    "反向传播的本质，其实就是梯度\n",
    "\n",
    "$ \\frac{dy}{dx} = \\frac{dy}{du}\\frac{du}{dx} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.0, shape=(), dtype=float32)\n",
      "tf.Tensor(2.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 链式法则\n",
    "x = tf.constant(1.)\n",
    "w1 = tf.constant(2.)\n",
    "b1 = tf.constant(1.)\n",
    "w2 = tf.constant(2.)\n",
    "b2 = tf.constant(1.)\n",
    "\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "\ttape.watch([w1, b1, w2, b2])\n",
    "\n",
    "\ty1 = x * w1 + b1\n",
    "\ty2 = y1 * w2 + b2\n",
    "\n",
    "dy2_dy1 = tape.gradient(y2, [y1])[0]\n",
    "dy1_dw1 = tape.gradient(y1, [w1])[0]\n",
    "dy2_dw1 = tape.gradient(y2, [w1])[0]\n",
    "\n",
    "\n",
    "print(dy2_dy1 * dy1_dw1)\n",
    "print(dy2_dw1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3.0, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 二阶梯度 （二次求导）\n",
    "w = tf.Variable(1.0)\n",
    "b = tf.Variable(2.0)\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as t1:\n",
    "  with tf.GradientTape() as t2:\n",
    "    y = x * w + b\n",
    "  dy_dw, dy_db = t2.gradient(y, [w, b])\n",
    "d2y_dw2 = t1.gradient(dy_dw, w)\n",
    "\n",
    "print(dy_dw)\n",
    "print(dy_db)\n",
    "print(d2y_dw2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 激活函数及其梯度\n",
    "#### Sigmoid / Logistic\n",
    "\n",
    "sigmoid很常用，但当x取值在[-3, 3]之外时也会遇到梯度离散的情况（梯度消失）\n",
    "\n",
    "常用于二分类且只输出1个值时（比如True的概率）;\n",
    "\n",
    "还常用于对预测值的限制上，比如Yolo。这是为了防止预测值过于离谱或发生突变。\n",
    "\n",
    "![sigmoid](./images/sigmoid.png)\n",
    "![sigmoid_derivative](./images/sigmoid_derivative.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-10.         -7.7777777  -5.5555553  -3.333333   -1.1111107   1.1111116\n",
      "   3.333334    5.5555563   7.7777786  10.       ], shape=(10,), dtype=float32)\n",
      "[<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
      "array([4.5395806e-05, 4.1859140e-04, 3.8362024e-03, 3.3258736e-02,\n",
      "       1.8632649e-01, 1.8632638e-01, 3.3258699e-02, 3.8362255e-03,\n",
      "       4.1860685e-04, 4.5416677e-05], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "# sigmoid\n",
    "a = tf.linspace(-10., 10., 10)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(a) # 因为调用了watch 所以不需要将a用tf.Variable包裹\n",
    "    y = tf.sigmoid(a)\n",
    "\n",
    "grads = tape.gradient(y, [a])\n",
    "print(a)\n",
    "print(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tanh\n",
    "在RNN中很常用\n",
    "![tanh](./images/tanh.png)\n",
    "![tanh_derivative](./images/tanh_derivative.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tanh\n",
    "a = tf.linspace(-5., 5., 10)\n",
    "tf.tanh(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rectified Linear Unit(ReLU)\n",
    "ReLU梯度很好计算，而且当>0时，梯度为1，最大程度的减少了梯度爆炸和梯度离散的发生。\n",
    "![relu](./images/relu.png)\n",
    "![relu_derivative](./images/relu_derivative.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       "array([-0.2       , -0.15555556, -0.11111112, -0.06666666, -0.02222222,\n",
       "        0.11111116,  0.33333337,  0.5555556 ,  0.7777778 ,  1.        ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#relu\n",
    "a = tf.linspace(-1., 1., 10)\n",
    "tf.nn.relu(a)\n",
    "# 当x<0时，会从某个值开始逐步降为接近0，防止梯度消失\n",
    "tf.nn.leaky_relu(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax\n",
    "常常和CrossEntropy一起使用，适用于多分类。它可以保证转换之后的值域之和为1。它可以让大的更大，小的更小。\n",
    "![softmax](./images/softmax_explain.png)\n",
    "![softmax_derivative](./images/softmax_derivative.png)\n",
    "![softmax_derivativeij](./images/softmax_derivativeij.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss及其梯度\n",
    "MSE与Cross Entropy Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w grad: tf.Tensor(\n",
      "[[-0.02904284  0.02904284]\n",
      " [-0.02101646  0.02101646]\n",
      " [ 0.19016582 -0.19016582]], shape=(3, 2), dtype=float32)\n",
      "b grad: tf.Tensor([ 0.25 -0.25], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# MSE gradient\n",
    "x=tf.random.normal([1,3])\n",
    "\n",
    "w=tf.ones([3,2])\n",
    "\n",
    "b=tf.ones([2])\n",
    "\n",
    "y = tf.constant([0, 1])\n",
    "\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "\n",
    "\ttape.watch([w, b])\n",
    "\n",
    "\t# logits = tf.sigmoid(x@w+b) \n",
    "\tlogits = tf.nn.softmax(x@w+b, axis=1)\n",
    "\tloss = tf.reduce_mean(tf.losses.MSE(y, logits))\n",
    "\n",
    "grads = tape.gradient(loss, [w, b])\n",
    "print('w grad:', grads[0])\n",
    "\n",
    "print('b grad:', grads[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w grad: tf.Tensor(\n",
      "[[ 0.00861339 -0.01581438  0.00720099]\n",
      " [ 0.02661874  0.12178067 -0.14839941]\n",
      " [ 0.02604978 -0.15760535  0.13155556]\n",
      " [-0.01489652  0.11392362 -0.09902708]], shape=(4, 3), dtype=float32)\n",
      "b grad: tf.Tensor([ 0.02462438 -0.05216555  0.02754116], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Crossentropy gradient\n",
    "tf.random.set_seed(4323)\n",
    "\n",
    "x=tf.random.normal([2,4])\n",
    "\n",
    "w=tf.random.normal([4,3])\n",
    "\n",
    "b=tf.random.normal([3])\n",
    "\n",
    "y = tf.constant([2, 1])\n",
    "\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "\n",
    "\ttape.watch([w, b])\n",
    "\tlogits = tf.nn.softmax(x@w+b, axis=1)\n",
    "\tloss = tf.reduce_mean(tf.losses.categorical_crossentropy(tf.one_hot(y, depth=3), logits, from_logits=True))\n",
    "\n",
    "grads = tape.gradient(loss, [w, b])\n",
    "print('w grad:', grads[0])\n",
    "print('b grad:', grads[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单层感知机及梯度  也就是最简单的1层神经网络  课时65\n",
    "\n",
    "$ y =  XW + b $\n",
    "\n",
    "$ \\frac{\\partial{E}}{\\partial{w_{j0}}} = (O_0 - t)O_0(1-O_0)x_j^0 $\n",
    "\n",
    "![single](./images/single_cell.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w grad: tf.Tensor(\n",
      "[[-0.0684463 ]\n",
      " [ 0.26229477]\n",
      " [ 0.3117205 ]], shape=(3, 1), dtype=float32)\n",
      "b grad: tf.Tensor([-0.15517864], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x=tf.random.normal([1,3])\n",
    "\n",
    "w=tf.ones([3,1])\n",
    "\n",
    "b=tf.ones([1])\n",
    "\n",
    "y = tf.constant([1])\n",
    "\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "\n",
    "\ttape.watch([w, b])\n",
    "\tlogits = tf.sigmoid(x@w+b) \n",
    "\tloss = tf.reduce_mean(tf.losses.MSE(y, logits))\n",
    "\n",
    "grads = tape.gradient(loss, [w, b])\n",
    "print('w grad:', grads[0])\n",
    "\n",
    "print('b grad:', grads[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多输出感知机及梯度 课时66\n",
    "\n",
    "在梯度计算中，主要点是，由于是求和，只有相关的权重求导，有效，其他支线的权重求导都是0.\n",
    "\n",
    "$ \\frac{\\partial{E}}{\\partial{w_{jk}}} = (O_k - t_k)O_k(1-O_k)x_j^0 $\n",
    "\n",
    "![multiple](./images/multiple_cell.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w grad: tf.Tensor(\n",
      "[[-0.06555234  0.04063531  0.02491703]\n",
      " [ 0.03510559  0.01809573 -0.05320131]\n",
      " [ 0.18592252 -0.09184361 -0.09407891]\n",
      " [-0.03733121 -0.02389582  0.06122703]], shape=(4, 3), dtype=float32)\n",
      "b grad: tf.Tensor([-0.03703704  0.07407407 -0.03703704], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x=tf.random.normal([2, 4])\n",
    "\n",
    "w=tf.ones([4, 3])\n",
    "\n",
    "b=tf.ones([3])\n",
    "\n",
    "y = tf.constant([2, 0])\n",
    "\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "\n",
    "\ttape.watch([w, b])\n",
    "\t# x@w+b 得到的shape是[b, 3], b是batch, 我们需要在axis=1维度上进行softmax\n",
    "\tlogits = tf.nn.softmax(x@w+b, axis=1)\n",
    "\tloss = tf.reduce_mean(tf.losses.MSE(tf.one_hot(y, depth=3), logits))\n",
    "\n",
    "grads = tape.gradient(loss, [w, b])\n",
    "print('w grad:', grads[0])\n",
    "\n",
    "print('b grad:', grads[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反向传播  多(K)层感知机\n",
    "\n",
    "$ \\frac{\\partial{E}}{\\partial{W_{jk}}} = (O_k - t_k)O_k(1-O_k)O_j^J $\n",
    "\n",
    "$ \\delta_k^K = (O_k - t_k)O_k(1-O_k) $\n",
    "\n",
    "$ \\frac{\\partial{E}}{\\partial{W_{jk}}} = \\delta_k^KO_j^J $\n",
    "\n",
    "$ \\frac{\\partial{E}}{\\partial{W_{ij}}} = O_j(1 - O_j)O_i\\sum_{k\\in{K}}((O_k - t_k)O_k(1-O_k)W_{jk}) $\n",
    "\n",
    "$ \\frac{\\partial{E}}{\\partial{W_{ij}}} = O_j(1 - O_j)O_i\\sum_{k\\in{K}}(\\delta_kW_{jk}) $\n",
    "\n",
    "图中$x_n^J$是$x_2^0$与$w_{ij}^J$的加权求和， $O_n^J$是$x_n^J$经过sigma函数得到的\n",
    "\n",
    "![multi-layer](./images/multi_layer.png)\n",
    "\n",
    "![bp-conclusion](./images/bp-conclusion.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('new_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d58ff1fa528449bec86ac3b87e5b9edec9a0f46f3f9c706338d5512923abfa3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
