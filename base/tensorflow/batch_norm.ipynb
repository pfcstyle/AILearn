{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为什么需要Normalization\n",
    "\n",
    "1. 对于一些激活函数，比如sigmoid，当值域小到或者达到一定范围，会有梯度离散的情况，那么这时候就有必要对数据进行标准化。\n",
    "2. 不同的变量，值域相差过大，会导致做优化时，路径难以选择。\n",
    "   \n",
    "   ![batch_norm](./images/batch_norm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般来说，神经网络进行训练的输入值，我们都希望其范围在0周围对称分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, mean, std):\n",
    "    x = x - mean\n",
    "    x = x / std\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Norm\n",
    "\n",
    "Batch Norm最重要的是动态更新，在每次反向传播时，都会对$\\gamma$和$\\beta$进行更新\n",
    "\n",
    "![batch norm](./images/batch_norm_all.png)\n",
    "\n",
    "主要关注公式里的$\\gamma$和$\\beta$，下图中，$\\mu$和$\\sigma$是统计量，不会参与更新；$\\gamma$和$\\beta$会在反向传播时，作为参数，做梯度运算，训练出最适合的数据normalization的均值和方差\n",
    "\n",
    "![pipline](./images/bm_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward in test mode: [<tf.Variable 'batch_normalization/gamma:0' shape=(3,) dtype=float32, numpy=array([1., 1., 1.], dtype=float32)>, <tf.Variable 'batch_normalization/beta:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>, <tf.Variable 'batch_normalization/moving_mean:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>, <tf.Variable 'batch_normalization/moving_variance:0' shape=(3,) dtype=float32, numpy=array([1., 1., 1.], dtype=float32)>]\n",
      "forward in train mode(1 step): [<tf.Variable 'batch_normalization/gamma:0' shape=(3,) dtype=float32, numpy=array([1., 1., 1.], dtype=float32)>, <tf.Variable 'batch_normalization/beta:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>, <tf.Variable 'batch_normalization/moving_mean:0' shape=(3,) dtype=float32, numpy=array([0.01052996, 0.00967027, 0.01049235], dtype=float32)>, <tf.Variable 'batch_normalization/moving_variance:0' shape=(3,) dtype=float32, numpy=array([0.9926309 , 0.99292   , 0.99182934], dtype=float32)>]\n",
      "forward in train mode(100 steps): [<tf.Variable 'batch_normalization/gamma:0' shape=(3,) dtype=float32, numpy=array([1., 1., 1.], dtype=float32)>, <tf.Variable 'batch_normalization/beta:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>, <tf.Variable 'batch_normalization/moving_mean:0' shape=(3,) dtype=float32, numpy=array([0.6714193, 0.6166031, 0.6690212], dtype=float32)>, <tf.Variable 'batch_normalization/moving_variance:0' shape=(3,) dtype=float32, numpy=array([0.5301253 , 0.54855824, 0.47901526], dtype=float32)>]\n",
      "backward(10 steps): [<tf.Variable 'batch_normalization/gamma:0' shape=(3,) dtype=float32, numpy=array([0.93554354, 0.9355193 , 0.93565047], dtype=float32)>, <tf.Variable 'batch_normalization/beta:0' shape=(3,) dtype=float32, numpy=array([-1.0468065e-08, -1.6181728e-09,  7.8231102e-09], dtype=float32)>, <tf.Variable 'batch_normalization/moving_mean:0' shape=(3,) dtype=float32, numpy=array([0.7079047 , 0.65010977, 0.70537627], dtype=float32)>, <tf.Variable 'batch_normalization/moving_variance:0' shape=(3,) dtype=float32, numpy=array([0.5045919 , 0.52402645, 0.4507045 ], dtype=float32)>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Code\\python\\AILearn\\env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers\n",
    "\n",
    "\n",
    "# 2 images with 4x4 size, 3 channels\n",
    "# we explicitly enforce the mean and stddev to N(1, 0.5)\n",
    "x = tf.random.normal([2,4,4,3], mean=1.,stddev=0.5)\n",
    "# center主要影响偏移量, 对应beta，scale主要影响缩放量, 对应gamma，\n",
    "# beta， gamma在前向传播时，不会进行更新，只是在反向传播时更新\n",
    "# trainable 表示是训练模式还是测试模式，如果是训练模式，会对moving_mean， moving_variance两个统计量进行更新\n",
    "net = layers.BatchNormalization(axis=-1, center=True, scale=True,\n",
    "                                trainable=True)\n",
    "\n",
    "out = net(x)\n",
    "# variables包含gamma, beta, moving_mean, moving_variance, 后两个是统计的全局均值和方差\n",
    "# trainable_variables只包含gamma和beta \n",
    "print('forward in test mode:', net.variables)\n",
    "\n",
    "\n",
    "out = net(x, training=True)\n",
    "print('forward in train mode(1 step):', net.variables)\n",
    "\n",
    "for i in range(100):\n",
    "    out = net(x, training=True)\n",
    "print('forward in train mode(100 steps):', net.variables)\n",
    "\n",
    "\n",
    "optimizer = optimizers.SGD(lr=1e-2)\n",
    "for i in range(10):\n",
    "    with tf.GradientTape() as tape:\n",
    "        out = net(x, training=True)\n",
    "        loss = tf.reduce_mean(tf.pow(out,2)) - 1\n",
    "\n",
    "    grads = tape.gradient(loss, net.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, net.trainable_variables))\n",
    "print('backward(10 steps):', net.variables)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bafc338abc57834bacf3c306014ce0c45f0aeaf201b721c6f2bfe17ccf7009fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
