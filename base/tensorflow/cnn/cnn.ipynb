{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积神经网络\n",
    "\n",
    "由于深度神经网络层数多，图片一般维度size又比较大，导致中间参数、参数梯度、样本图片的内存占用非常大，所以一般无法直接使用Dense层进行全连接，所以才出现了卷积的概念。\n",
    "\n",
    "![receptive_field](../images/receptive_field.png)\n",
    "\n",
    "\n",
    "### Convolution(卷积)\n",
    "\n",
    "图中‘*’表示卷积运算， 下图的三角形是y(t)的运算结果图。下图是连续函数的卷积，所以使用的是积分运算；平时神经网络中是离散值，我们使用$\\sum$\n",
    "\n",
    "3层3 x 3的卷积核的感受野与7 x 7相同，但是一般选择3层3 x 3的卷积核，这是因为：参数少，特征多，速度快(NVDIA底层优化过)，增加了非线性。\n",
    "\n",
    "卷积核本质就是权重矩阵，也是更新变化的。\n",
    "\n",
    "![convolution](../images/convolution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution in Computer Vision\n",
    "\n",
    "#### sharpen(锐化)\n",
    "\n",
    "![sharpen](../images/sharpen.png)\n",
    "\n",
    "#### blur(模糊)\n",
    "\n",
    "![blur](../images/blur.png)\n",
    "\n",
    "#### Edge Detect(边缘检测)\n",
    "\n",
    "![edge_detect](../images/edge_detect.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Padding & Stride\n",
    "\n",
    "卷积可以增加padding(填充0)，可以更好的保留特征，并且得到和原始图片size相同的输出；卷积移动时，可以通过调整Stride（步长）进行降维，详见右图；通过调整padding和Stride可以得到自己想要size的输出。\n",
    "\n",
    "![padding_stride](../images/padding_stride.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Channels\n",
    "\n",
    "可以使用多个kernel进行卷积，从而得到多通道输出。注意，channel数必须和input的channel相同，比如input:7*7*3是3通道，卷积也应该是3通道。每一个卷积之后的结果可以再添加一个不同bias\n",
    "\n",
    "![channels](../images/channels.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers.Conv2D\n",
    "# 4是channel，padding: valid是不padding； same是自动padding，让output大小和原图相同\n",
    "layer = layers.Conv2D(4, kernel_size=5, strides=1, padding='valid')\n",
    "# out = layer(x) layer.kernel, layer.bias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积参数共享\n",
    "\n",
    "卷积的参数在单次迭代中，针对图像的不同位置是共享的。实际上，针对图像的不同位置，应该使用不同的卷积核来提取，但这样参数量太大，而且容易导致过拟合。所以，一般采用多个卷积核但是参数共享来用。\n",
    "\n",
    "![convolution shard](../images/convolution-shared.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LENET 经典卷积神经网络\n",
    "\n",
    "图中标注的数字是输出的维度\n",
    "\n",
    "![lenet](../images/lenet.png)\n",
    "\n",
    "#### Subsampling（下采样）（池化）\n",
    "一般常见Max/Avg池化层\n",
    "\n",
    "![pooling](../images/pooling.png)\n",
    "\n",
    "#### UpSampling 2D\n",
    "升维，把一个比较小的图片放大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooling\n",
    "pool_layer = layers.MaxPool2D(2, strides=2)\n",
    "\n",
    "# UpSampling 2D\n",
    "x = tf.random.normal([1, 7, 7, 4])\n",
    "layer = layers.UpSampling2D(size=3)\n",
    "out = layer(x) #shape=[1, 21, 21, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常见CNN\n",
    "\n",
    "#### AlexNet\n",
    "将准确率提高了10%，主要影响力大， 当时采用了两个GPU并行的训练方式，但其网络结构对于现在的深度学习网络（GPU更强大了），已经没有多大参考性。\n",
    "\n",
    "采用11 x 11卷积\n",
    "\n",
    "![AlexNet](../images/AlexNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG\n",
    "\n",
    "VGG的探索发现，卷积核缩小，不会降低准确度，同时又会大大降低计算量，可以使用1x1和3x3卷积\n",
    "\n",
    "![VGG](../images/vgg.png)\n",
    "\n",
    "其中1x1的卷积，可以用来降维，比如32 * 14 * 14 通过[16 * 1 * 1]的kernel，可以降到16 * 14 * 14，这同时保证了图片大小不变\n",
    "\n",
    "![kernel1](../images/kernal1x1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GoogleNet\n",
    "\n",
    "探索出一层可以使用多个种类的卷积核， 多个卷积核卷积之后的结果通过Filter concatenation进行归一得到相同size的结果。googlenet一共22层。googlenet最多到22层，如果再增加层数，由于训练上的难度大大增加，导致准确率增加非常缓慢，效果变差。\n",
    "\n",
    "![multi_kernel](../images/multi_kernel.png)\n",
    "\n",
    "![googlenet](../images/googlenet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('new_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d58ff1fa528449bec86ac3b87e5b9edec9a0f46f3f9c706338d5512923abfa3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
