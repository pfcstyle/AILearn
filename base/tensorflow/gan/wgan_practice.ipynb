{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  tensorflow as tf\n",
    "from    tensorflow import keras\n",
    "from    keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # z: [b, 100] => [b, 3*3*512] => [b, 3, 3, 512] => [b, 64, 64, 3]\n",
    "        self.fc = layers.Dense(3*3*512)\n",
    "\n",
    "        self.conv1 = layers.Conv2DTranspose(256, 3, 3, 'valid')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "\n",
    "        self.conv2 = layers.Conv2DTranspose(128, 5, 2, 'valid')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "\n",
    "        self.conv3 = layers.Conv2DTranspose(3, 4, 3, 'valid')\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # [z, 100] => [z, 3*3*512]\n",
    "        x = self.fc(inputs)\n",
    "        x = tf.reshape(x, [-1, 3, 3, 512])\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "\n",
    "        #\n",
    "        x = tf.nn.leaky_relu(self.bn1(self.conv1(x), training=training))\n",
    "        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training=training))\n",
    "        x = self.conv3(x)\n",
    "        x = tf.tanh(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # [b, 64, 64, 3] => [b, 1]\n",
    "        self.conv1 = layers.Conv2D(64, 5, 3, 'valid')\n",
    "\n",
    "        # 这里激活函数不适用relu,因为x<0时，梯度为0\n",
    "        self.conv2 = layers.Conv2D(128, 5, 3, 'valid')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "\n",
    "        self.conv3 = layers.Conv2D(256, 5, 3, 'valid')\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "\n",
    "        # [b, h, w ,c] => [b, -1]\n",
    "        # 打平操作\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc = layers.Dense(1)\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # 这里激活函数不适用relu,因为x<0时，梯度为0，leaky_relu在小于0时，\n",
    "        # 会趋近于0（越接近0， rele值也越接近0）\n",
    "        x = tf.nn.leaky_relu(self.conv1(inputs))\n",
    "        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training=training))\n",
    "        x = tf.nn.leaky_relu(self.bn3(self.conv3(x), training=training))\n",
    "\n",
    "        # [b, h, w, c] => [b, -1]\n",
    "        x = self.flatten(x)\n",
    "        # [b, -1] => [b, 1]\n",
    "        logits = self.fc(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.02270677]\n",
      " [ 0.03103754]], shape=(2, 1), dtype=float32)\n",
      "(2, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "d = Discriminator()\n",
    "g = Generator()\n",
    "\n",
    "\n",
    "x = tf.random.normal([2, 64, 64, 3])\n",
    "z = tf.random.normal([2, 100])\n",
    "\n",
    "prob = d(x)\n",
    "print(prob)\n",
    "x_hat = g(z)\n",
    "print(x_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  os\n",
    "import  numpy as np\n",
    "import  tensorflow as tf\n",
    "from    tensorflow import keras\n",
    "from PIL import Image\n",
    "import  glob\n",
    "from    dataset import make_anime_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(val_out, val_block_size, image_path, color_mode):\n",
    "    def preprocess(img):\n",
    "        img = ((img + 1.0) * 127.5).astype(np.uint8)\n",
    "        # img = img.astype(np.uint8)\n",
    "        return img\n",
    "\n",
    "    preprocesed = preprocess(val_out)\n",
    "    final_image = np.array([])\n",
    "    single_row = np.array([])\n",
    "    for b in range(val_out.shape[0]):\n",
    "        # concat image into a row\n",
    "        if single_row.size == 0:\n",
    "            single_row = preprocesed[b, :, :, :]\n",
    "        else:\n",
    "            single_row = np.concatenate((single_row, preprocesed[b, :, :, :]), axis=1)\n",
    "\n",
    "        # concat image row to final_image\n",
    "        if (b+1) % val_block_size == 0:\n",
    "            if final_image.size == 0:\n",
    "                final_image = single_row\n",
    "            else:\n",
    "                final_image = np.concatenate((final_image, single_row), axis=0)\n",
    "\n",
    "            # reset single row\n",
    "            single_row = np.array([])\n",
    "\n",
    "    if final_image.shape[2] == 1:\n",
    "        final_image = np.squeeze(final_image, axis=2)\n",
    "    Image.fromarray(final_image).save(image_path)\n",
    "\n",
    "\n",
    "def celoss_ones(logits):\n",
    "    # [b, 1]\n",
    "    # [b] = [1, 1, 1, 1,]\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,\n",
    "                                                   labels=tf.ones_like(logits))\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "def celoss_zeros(logits):\n",
    "    # [b, 1]\n",
    "    # [b] = [1, 1, 1, 1,]\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,\n",
    "                                                   labels=tf.zeros_like(logits))\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def gradient_penalty(discriminator, batch_x, fake_image):\n",
    "\n",
    "    batchsz = batch_x.shape[0]\n",
    "\n",
    "    # [b, h, w, c]\n",
    "    t = tf.random.uniform([batchsz, 1, 1, 1])\n",
    "    # [b, 1, 1, 1] => [b, h, w, c]\n",
    "    t = tf.broadcast_to(t, batch_x.shape)\n",
    "\n",
    "    interplate = t * batch_x + (1 - t) * fake_image\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch([interplate])\n",
    "        d_interplote_logits = discriminator(interplate)\n",
    "    grads = tape.gradient(d_interplote_logits, interplate)\n",
    "\n",
    "    # grads:[b, h, w, c] => [b, -1]\n",
    "    grads = tf.reshape(grads, [grads.shape[0], -1])\n",
    "    gp = tf.norm(grads, axis=1) #[b]\n",
    "    gp = tf.reduce_mean( (gp-1)**2 )\n",
    "\n",
    "    return gp\n",
    "\n",
    "def d_loss_fn(generator, discriminator, batch_z, batch_x, is_training):\n",
    "    # 1. treat real image as real\n",
    "    # 2. treat generated image as fake\n",
    "    fake_image = generator(batch_z, is_training)\n",
    "    d_fake_logits = discriminator(fake_image, is_training)\n",
    "    d_real_logits = discriminator(batch_x, is_training)\n",
    "\n",
    "    d_loss_real = celoss_ones(d_real_logits)\n",
    "    d_loss_fake = celoss_zeros(d_fake_logits)\n",
    "    gp = gradient_penalty(discriminator, batch_x, fake_image)\n",
    "\n",
    "    loss = d_loss_fake + d_loss_real + 1. * gp\n",
    "\n",
    "    return loss, gp\n",
    "\n",
    "\n",
    "def g_loss_fn(generator, discriminator, batch_z, is_training):\n",
    "\n",
    "    fake_image = generator(batch_z, is_training)\n",
    "    d_fake_logits = discriminator(fake_image, is_training)\n",
    "    loss = celoss_ones(d_fake_logits)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=TensorSpec(shape=(512, 64, 64, 3), dtype=tf.float32, name=None)> (64, 64, 3)\n",
      "(512, 64, 64, 3) 1.0 -1.0\n",
      "0 d-loss: 2.3967576026916504 g-loss: 1.399315595626831 gp: 0.7930325269699097\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 48\u001b[0m\n\u001b[0;32m     44\u001b[0m d_optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, discriminator\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 48\u001b[0m     g_loss \u001b[38;5;241m=\u001b[39m \u001b[43mg_loss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_z\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_training\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(g_loss, generator\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[0;32m     50\u001b[0m g_optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, generator\u001b[38;5;241m.\u001b[39mtrainable_variables))\n",
      "Cell \u001b[1;32mIn[5], line 88\u001b[0m, in \u001b[0;36mg_loss_fn\u001b[1;34m(generator, discriminator, batch_z, is_training)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mg_loss_fn\u001b[39m(generator, discriminator, batch_z, is_training):\n\u001b[1;32m---> 88\u001b[0m     fake_image \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_z\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_training\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m     d_fake_logits \u001b[38;5;241m=\u001b[39m discriminator(fake_image, is_training)\n\u001b[0;32m     90\u001b[0m     loss \u001b[38;5;241m=\u001b[39m celoss_ones(d_fake_logits)\n",
      "File \u001b[1;32mf:\\Code\\python\\AILearn\\new_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mf:\\Code\\python\\AILearn\\new_env\\lib\\site-packages\\keras\\engine\\training.py:557\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39mcopied_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    555\u001b[0m     layout_map_lib\u001b[39m.\u001b[39m_map_subclass_model_variable(\u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout_map)\n\u001b[1;32m--> 557\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\Code\\python\\AILearn\\new_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mf:\\Code\\python\\AILearn\\new_env\\lib\\site-packages\\keras\\engine\\base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1096\u001b[0m ):\n\u001b[1;32m-> 1097\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mf:\\Code\\python\\AILearn\\new_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 25\u001b[0m, in \u001b[0;36mGenerator.call\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x), training\u001b[38;5;241m=\u001b[39mtraining))\n\u001b[1;32m---> 25\u001b[0m x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, training\u001b[38;5;241m=\u001b[39mtraining))\n\u001b[0;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x)\n\u001b[0;32m     27\u001b[0m x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtanh(x)\n",
      "File \u001b[1;32mf:\\Code\\python\\AILearn\\new_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mf:\\Code\\python\\AILearn\\new_env\\lib\\site-packages\\keras\\engine\\base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1096\u001b[0m ):\n\u001b[1;32m-> 1097\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mf:\\Code\\python\\AILearn\\new_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Code\\python\\AILearn\\new_env\\lib\\site-packages\\keras\\layers\\convolutional\\conv2d_transpose.py:296\u001b[0m, in \u001b[0;36mConv2DTranspose.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    293\u001b[0m     output_shape \u001b[39m=\u001b[39m (batch_size, out_height, out_width, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilters)\n\u001b[0;32m    295\u001b[0m output_shape_tensor \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mstack(output_shape)\n\u001b[1;32m--> 296\u001b[0m outputs \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39;49mconv2d_transpose(\n\u001b[0;32m    297\u001b[0m     inputs,\n\u001b[0;32m    298\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel,\n\u001b[0;32m    299\u001b[0m     output_shape_tensor,\n\u001b[0;32m    300\u001b[0m     strides\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrides,\n\u001b[0;32m    301\u001b[0m     padding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[0;32m    302\u001b[0m     data_format\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_format,\n\u001b[0;32m    303\u001b[0m     dilation_rate\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation_rate,\n\u001b[0;32m    304\u001b[0m )\n\u001b[0;32m    306\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tf\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    307\u001b[0m     \u001b[39m# Infer the static output shape:\u001b[39;00m\n\u001b[0;32m    308\u001b[0m     out_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_output_shape(inputs\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mf:\\Code\\python\\AILearn\\new_env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mf:\\Code\\python\\AILearn\\new_env\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mf:\\Code\\python\\AILearn\\new_env\\lib\\site-packages\\keras\\backend.py:6119\u001b[0m, in \u001b[0;36mconv2d_transpose\u001b[1;34m(x, kernel, output_shape, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[0;32m   6116\u001b[0m     strides \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m strides\n\u001b[0;32m   6118\u001b[0m \u001b[39mif\u001b[39;00m dilation_rate \u001b[39m==\u001b[39m (\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m):\n\u001b[1;32m-> 6119\u001b[0m     x \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mcompat\u001b[39m.\u001b[39;49mv1\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mconv2d_transpose(\n\u001b[0;32m   6120\u001b[0m         x,\n\u001b[0;32m   6121\u001b[0m         kernel,\n\u001b[0;32m   6122\u001b[0m         output_shape,\n\u001b[0;32m   6123\u001b[0m         strides,\n\u001b[0;32m   6124\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[0;32m   6125\u001b[0m         data_format\u001b[39m=\u001b[39;49mtf_data_format,\n\u001b[0;32m   6126\u001b[0m     )\n\u001b[0;32m   6127\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   6128\u001b[0m     \u001b[39mif\u001b[39;00m dilation_rate[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m dilation_rate[\u001b[39m1\u001b[39m]:\n",
      "File \u001b[1;32mf:\\Code\\python\\AILearn\\new_env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mf:\\Code\\python\\AILearn\\new_env\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mf:\\Code\\python\\AILearn\\new_env\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:2675\u001b[0m, in \u001b[0;36mconv2d_transpose\u001b[1;34m(value, filter, output_shape, strides, padding, data_format, name, input, filters, dilations)\u001b[0m\n\u001b[0;32m   2672\u001b[0m \u001b[39mfilter\u001b[39m \u001b[39m=\u001b[39m deprecated_argument_lookup(\u001b[39m\"\u001b[39m\u001b[39mfilters\u001b[39m\u001b[39m\"\u001b[39m, filters, \u001b[39m\"\u001b[39m\u001b[39mfilter\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mfilter\u001b[39m)\n\u001b[0;32m   2673\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(name, \u001b[39m\"\u001b[39m\u001b[39mconv2d_transpose\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2674\u001b[0m                     [value, \u001b[39mfilter\u001b[39m, output_shape]) \u001b[39mas\u001b[39;00m name:\n\u001b[1;32m-> 2675\u001b[0m   \u001b[39mreturn\u001b[39;00m conv2d_transpose_v2(\n\u001b[0;32m   2676\u001b[0m       value,\n\u001b[0;32m   2677\u001b[0m       \u001b[39mfilter\u001b[39;49m,\n\u001b[0;32m   2678\u001b[0m       output_shape,\n\u001b[0;32m   2679\u001b[0m       strides,\n\u001b[0;32m   2680\u001b[0m       padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[0;32m   2681\u001b[0m       data_format\u001b[39m=\u001b[39;49mdata_format,\n\u001b[0;32m   2682\u001b[0m       dilations\u001b[39m=\u001b[39;49mdilations,\n\u001b[0;32m   2683\u001b[0m       name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mf:\\Code\\python\\AILearn\\new_env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mf:\\Code\\python\\AILearn\\new_env\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mf:\\Code\\python\\AILearn\\new_env\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:2761\u001b[0m, in \u001b[0;36mconv2d_transpose_v2\u001b[1;34m(input, filters, output_shape, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   2758\u001b[0m dilations \u001b[39m=\u001b[39m _get_sequence(dilations, \u001b[39m2\u001b[39m, channel_index, \u001b[39m\"\u001b[39m\u001b[39mdilations\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2759\u001b[0m padding, explicit_paddings \u001b[39m=\u001b[39m convert_padding(padding)\n\u001b[1;32m-> 2761\u001b[0m \u001b[39mreturn\u001b[39;00m gen_nn_ops\u001b[39m.\u001b[39;49mconv2d_backprop_input(\n\u001b[0;32m   2762\u001b[0m     input_sizes\u001b[39m=\u001b[39;49moutput_shape,\n\u001b[0;32m   2763\u001b[0m     \u001b[39mfilter\u001b[39;49m\u001b[39m=\u001b[39;49mfilters,\n\u001b[0;32m   2764\u001b[0m     out_backprop\u001b[39m=\u001b[39;49m\u001b[39minput\u001b[39;49m,\n\u001b[0;32m   2765\u001b[0m     strides\u001b[39m=\u001b[39;49mstrides,\n\u001b[0;32m   2766\u001b[0m     padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[0;32m   2767\u001b[0m     explicit_paddings\u001b[39m=\u001b[39;49mexplicit_paddings,\n\u001b[0;32m   2768\u001b[0m     data_format\u001b[39m=\u001b[39;49mdata_format,\n\u001b[0;32m   2769\u001b[0m     dilations\u001b[39m=\u001b[39;49mdilations,\n\u001b[0;32m   2770\u001b[0m     name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mf:\\Code\\python\\AILearn\\new_env\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py:1415\u001b[0m, in \u001b[0;36mconv2d_backprop_input\u001b[1;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1413\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   1414\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1415\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   1416\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mConv2DBackpropInput\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, input_sizes, \u001b[39mfilter\u001b[39;49m, out_backprop,\n\u001b[0;32m   1417\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mstrides\u001b[39;49m\u001b[39m\"\u001b[39;49m, strides, \u001b[39m\"\u001b[39;49m\u001b[39muse_cudnn_on_gpu\u001b[39;49m\u001b[39m\"\u001b[39;49m, use_cudnn_on_gpu, \u001b[39m\"\u001b[39;49m\u001b[39mpadding\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1418\u001b[0m       padding, \u001b[39m\"\u001b[39;49m\u001b[39mexplicit_paddings\u001b[39;49m\u001b[39m\"\u001b[39;49m, explicit_paddings, \u001b[39m\"\u001b[39;49m\u001b[39mdata_format\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1419\u001b[0m       data_format, \u001b[39m\"\u001b[39;49m\u001b[39mdilations\u001b[39;49m\u001b[39m\"\u001b[39;49m, dilations)\n\u001b[0;32m   1420\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   1421\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(22)\n",
    "np.random.seed(22)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "assert tf.__version__.startswith('2.')\n",
    "\n",
    "\n",
    "# hyper parameters\n",
    "z_dim = 100\n",
    "epochs = 3000000\n",
    "batch_size = 512\n",
    "learning_rate = 0.002\n",
    "is_training = True\n",
    "\n",
    "\n",
    "img_path = glob.glob(r'faces\\*.jpg')\n",
    "\n",
    "dataset, img_shape, _ = make_anime_dataset(img_path, batch_size)\n",
    "print(dataset, img_shape)\n",
    "sample = next(iter(dataset))\n",
    "print(sample.shape, tf.reduce_max(sample).numpy(),\n",
    "        tf.reduce_min(sample).numpy())\n",
    "dataset = dataset.repeat()\n",
    "db_iter = iter(dataset)\n",
    "\n",
    "\n",
    "generator = Generator()\n",
    "generator.build(input_shape = (None, z_dim))\n",
    "discriminator = Discriminator()\n",
    "discriminator.build(input_shape=(None, 64, 64, 3))\n",
    "\n",
    "g_optimizer = tf.optimizers.Adam(learning_rate=learning_rate, beta_1=0.5)\n",
    "d_optimizer = tf.optimizers.Adam(learning_rate=learning_rate, beta_1=0.5)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    batch_z = tf.random.uniform([batch_size, z_dim], minval=-1., maxval=1.)\n",
    "    batch_x = next(db_iter)\n",
    "\n",
    "    # train D\n",
    "    with tf.GradientTape() as tape:\n",
    "        d_loss, gp = d_loss_fn(generator, discriminator, batch_z, batch_x, is_training)\n",
    "    grads = tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "    d_optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n",
    "\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        g_loss = g_loss_fn(generator, discriminator, batch_z, is_training)\n",
    "    grads = tape.gradient(g_loss, generator.trainable_variables)\n",
    "    g_optimizer.apply_gradients(zip(grads, generator.trainable_variables))\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, 'd-loss:',float(d_loss), 'g-loss:', float(g_loss),\n",
    "                'gp:', float(gp))\n",
    "\n",
    "        z = tf.random.uniform([100, z_dim])\n",
    "        fake_image = generator(z, training=False)\n",
    "        img_path = os.path.join('images', 'wgan-%d.png'%epoch)\n",
    "        save_result(fake_image.numpy(), 10, img_path, color_mode='P')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('new_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d58ff1fa528449bec86ac3b87e5b9edec9a0f46f3f9c706338d5512923abfa3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
