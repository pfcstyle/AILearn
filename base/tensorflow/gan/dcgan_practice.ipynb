{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  tensorflow as tf\n",
    "from    tensorflow import keras\n",
    "from    keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # z: [b, 100] => [b, 3*3*512] => [b, 3, 3, 512] => [b, 64, 64, 3]\n",
    "        self.fc = layers.Dense(3*3*512)\n",
    "        # Conv2DTranspose是Conv的逆向，输出比输入维度大，通过padding和stride实现\n",
    "        # output: kernal_size\n",
    "        # 输出维度： (input_size - 1) * strides - 2 * padding + kernal_size\n",
    "        self.conv1 = layers.Conv2DTranspose(256, 3, 3, 'valid') # 9 * 9\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "\n",
    "        self.conv2 = layers.Conv2DTranspose(128, 5, 2, 'valid') # 21\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "\n",
    "        self.conv3 = layers.Conv2DTranspose(3, 4, 3, 'valid') # 64\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # [z, 100] => [z, 3*3*512]\n",
    "        x = self.fc(inputs)\n",
    "        x = tf.reshape(x, [-1, 3, 3, 512])\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        #\n",
    "        x = tf.nn.leaky_relu(self.bn1(self.conv1(x), training=training))\n",
    "        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training=training))\n",
    "        x = self.conv3(x)\n",
    "        x = tf.tanh(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # [b, 64, 64, 3] => [b, 1]\n",
    "        # 输出维度(valid)： floor((input_size - kernal_size)/strides + 1)\n",
    "        # 输出维度(same)： ceil(input_size/strides)\n",
    "        self.conv1 = layers.Conv2D(64, 5, 3, 'valid')\n",
    "\n",
    "        # 这里激活函数不适用relu,因为x<0时，梯度为0\n",
    "        self.conv2 = layers.Conv2D(128, 5, 3, 'valid')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "\n",
    "        self.conv3 = layers.Conv2D(256, 5, 3, 'valid')\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "\n",
    "        # [b, h, w ,c] => [b, -1]\n",
    "        # 打平操作\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc = layers.Dense(1)\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # 这里激活函数不适用relu,因为x<0时，梯度为0，leaky_relu在小于0时，\n",
    "        # 会趋近于0（越接近0， rele值也越接近0）\n",
    "        x = tf.nn.leaky_relu(self.conv1(inputs))\n",
    "        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training=training))\n",
    "        x = tf.nn.leaky_relu(self.bn3(self.conv3(x), training=training))\n",
    "\n",
    "        # [b, h, w, c] => [b, -1]\n",
    "        x = self.flatten(x)\n",
    "        # [b, -1] => [b, 1]\n",
    "        logits = self.fc(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.15043981]\n",
      " [0.07876404]], shape=(2, 1), dtype=float32)\n",
      "(2, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "d = Discriminator()\n",
    "g = Generator()\n",
    "\n",
    "\n",
    "x = tf.random.normal([2, 64, 64, 3])\n",
    "z = tf.random.normal([2, 100])\n",
    "\n",
    "prob = d(x)\n",
    "print(prob)\n",
    "x_hat = g(z)\n",
    "print(x_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  os\n",
    "import  numpy as np\n",
    "import  tensorflow as tf\n",
    "from    tensorflow import keras\n",
    "from PIL import Image\n",
    "import  glob\n",
    "from    dataset import make_anime_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(val_out, val_block_size, image_path, color_mode):\n",
    "    def preprocess(img):\n",
    "        img = ((img + 1.0) * 127.5).astype(np.uint8)\n",
    "        # img = img.astype(np.uint8)\n",
    "        return img\n",
    "\n",
    "    preprocesed = preprocess(val_out)\n",
    "    final_image = np.array([])\n",
    "    single_row = np.array([])\n",
    "    for b in range(val_out.shape[0]):\n",
    "        # concat image into a row\n",
    "        if single_row.size == 0:\n",
    "            single_row = preprocesed[b, :, :, :]\n",
    "        else:\n",
    "            single_row = np.concatenate((single_row, preprocesed[b, :, :, :]), axis=1)\n",
    "\n",
    "        # concat image row to final_image\n",
    "        if (b+1) % val_block_size == 0:\n",
    "            if final_image.size == 0:\n",
    "                final_image = single_row\n",
    "            else:\n",
    "                final_image = np.concatenate((final_image, single_row), axis=0)\n",
    "\n",
    "            # reset single row\n",
    "            single_row = np.array([])\n",
    "\n",
    "    if final_image.shape[2] == 1:\n",
    "        final_image = np.squeeze(final_image, axis=2)\n",
    "    Image.fromarray(final_image).save(image_path)\n",
    "\n",
    "# 计算真图片的loss\n",
    "def celoss_ones(logits):\n",
    "    # [b, 1]\n",
    "    # [b] = [1, 1, 1, 1,]\n",
    "    # 使用sigmoid将输出转换为[0, 1]\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,\n",
    "                                                   labels=tf.ones_like(logits))\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# 计算假图片的loss\n",
    "def celoss_zeros(logits):\n",
    "    # [b, 1]\n",
    "    # [b] = [1, 1, 1, 1,]\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,\n",
    "                                                   labels=tf.zeros_like(logits))\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def d_loss_fn(generator, discriminator, batch_z, batch_x, is_training):\n",
    "    # 1. treat real image as real\n",
    "    # 2. treat generated image as fake\n",
    "    fake_image = generator(batch_z, is_training)\n",
    "    d_fake_logits = discriminator(fake_image, is_training)\n",
    "    d_real_logits = discriminator(batch_x, is_training)\n",
    "\n",
    "    d_loss_real = celoss_ones(d_real_logits)\n",
    "    d_loss_fake = celoss_zeros(d_fake_logits)\n",
    "\n",
    "    loss = d_loss_fake + d_loss_real\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def g_loss_fn(generator, discriminator, batch_z, is_training):\n",
    "\n",
    "    fake_image = generator(batch_z, is_training)\n",
    "    d_fake_logits = discriminator(fake_image, is_training)\n",
    "    loss = celoss_ones(d_fake_logits)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=TensorSpec(shape=(512, 64, 64, 3), dtype=tf.float32, name=None)> (64, 64, 3)\n",
      "(512, 64, 64, 3) 1.0 -1.0\n",
      "0 d-loss: 1.5381639003753662 g-loss: 1.76137113571167\n",
      "100 d-loss: 1.371740698814392 g-loss: 0.8220462799072266\n",
      "200 d-loss: 1.3590083122253418 g-loss: 0.7357417345046997\n",
      "300 d-loss: 1.3474998474121094 g-loss: 0.8031489253044128\n",
      "400 d-loss: 1.3607661724090576 g-loss: 0.7511063814163208\n",
      "500 d-loss: 1.3735337257385254 g-loss: 0.829103946685791\n",
      "600 d-loss: 1.335923194885254 g-loss: 0.8518822193145752\n",
      "700 d-loss: 1.329111099243164 g-loss: 0.7549296617507935\n",
      "800 d-loss: 1.328869342803955 g-loss: 0.8306502103805542\n",
      "900 d-loss: 1.32321298122406 g-loss: 0.8544598817825317\n",
      "1000 d-loss: 1.3676694631576538 g-loss: 0.8505797982215881\n",
      "1100 d-loss: 1.3882262706756592 g-loss: 0.9367552995681763\n",
      "1200 d-loss: 1.3174521923065186 g-loss: 0.925320029258728\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m     48\u001b[0m     g_loss \u001b[38;5;241m=\u001b[39m g_loss_fn(generator, discriminator, batch_z, is_training)\n\u001b[1;32m---> 49\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m g_optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, generator\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mf:\\Code\\python\\AILearn\\new_env\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1113\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1107\u001b[0m   output_gradients \u001b[39m=\u001b[39m (\n\u001b[0;32m   1108\u001b[0m       composite_tensor_gradient\u001b[39m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[0;32m   1109\u001b[0m           output_gradients))\n\u001b[0;32m   1110\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m   1111\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m output_gradients]\n\u001b[1;32m-> 1113\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39;49mimperative_grad(\n\u001b[0;32m   1114\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tape,\n\u001b[0;32m   1115\u001b[0m     flat_targets,\n\u001b[0;32m   1116\u001b[0m     flat_sources,\n\u001b[0;32m   1117\u001b[0m     output_gradients\u001b[39m=\u001b[39;49moutput_gradients,\n\u001b[0;32m   1118\u001b[0m     sources_raw\u001b[39m=\u001b[39;49mflat_sources_raw,\n\u001b[0;32m   1119\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49munconnected_gradients)\n\u001b[0;32m   1121\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[0;32m   1122\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[0;32m   1123\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
      "File \u001b[1;32mf:\\Code\\python\\AILearn\\new_env\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     65\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[1;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeGradient(\n\u001b[0;32m     68\u001b[0m     tape\u001b[39m.\u001b[39;49m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m     69\u001b[0m     target,\n\u001b[0;32m     70\u001b[0m     sources,\n\u001b[0;32m     71\u001b[0m     output_gradients,\n\u001b[0;32m     72\u001b[0m     sources_raw,\n\u001b[0;32m     73\u001b[0m     compat\u001b[39m.\u001b[39;49mas_str(unconnected_gradients\u001b[39m.\u001b[39;49mvalue))\n",
      "File \u001b[1;32mf:\\Code\\python\\AILearn\\new_env\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:160\u001b[0m, in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    158\u001b[0m     gradient_name_scope \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m forward_pass_name_scope \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[1;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39;49mout_grads)\n\u001b[0;32m    161\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m   \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39mout_grads)\n",
      "File \u001b[1;32mf:\\Code\\python\\AILearn\\new_env\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py:45\u001b[0m, in \u001b[0;36m_Conv2DBackpropInputGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39m\"\"\"The derivatives for deconvolution.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \n\u001b[0;32m     34\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39m  the gradients w.r.t. the input and the filter\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[39m# We call the gen_nn_ops backprop functions instead of nn_ops backprop\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39m# functions for performance reasons in Eager mode. See _Conv2DGrad.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m     44\u001b[0m     \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m---> 45\u001b[0m     gen_nn_ops\u001b[39m.\u001b[39;49mconv2d_backprop_filter(\n\u001b[0;32m     46\u001b[0m         grad,\n\u001b[0;32m     47\u001b[0m         array_ops\u001b[39m.\u001b[39;49mshape(op\u001b[39m.\u001b[39;49minputs[\u001b[39m1\u001b[39;49m]),\n\u001b[0;32m     48\u001b[0m         op\u001b[39m.\u001b[39;49minputs[\u001b[39m2\u001b[39;49m],\n\u001b[0;32m     49\u001b[0m         dilations\u001b[39m=\u001b[39;49mop\u001b[39m.\u001b[39;49mget_attr(\u001b[39m\"\u001b[39;49m\u001b[39mdilations\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m     50\u001b[0m         strides\u001b[39m=\u001b[39;49mop\u001b[39m.\u001b[39;49mget_attr(\u001b[39m\"\u001b[39;49m\u001b[39mstrides\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m     51\u001b[0m         padding\u001b[39m=\u001b[39;49mop\u001b[39m.\u001b[39;49mget_attr(\u001b[39m\"\u001b[39;49m\u001b[39mpadding\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m     52\u001b[0m         explicit_paddings\u001b[39m=\u001b[39;49mop\u001b[39m.\u001b[39;49mget_attr(\u001b[39m\"\u001b[39;49m\u001b[39mexplicit_paddings\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m     53\u001b[0m         use_cudnn_on_gpu\u001b[39m=\u001b[39;49mop\u001b[39m.\u001b[39;49mget_attr(\u001b[39m\"\u001b[39;49m\u001b[39muse_cudnn_on_gpu\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m     54\u001b[0m         data_format\u001b[39m=\u001b[39;49mop\u001b[39m.\u001b[39;49mget_attr(\u001b[39m\"\u001b[39;49m\u001b[39mdata_format\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mdecode()),\n\u001b[0;32m     55\u001b[0m     gen_nn_ops\u001b[39m.\u001b[39mconv2d(\n\u001b[0;32m     56\u001b[0m         grad,\n\u001b[0;32m     57\u001b[0m         op\u001b[39m.\u001b[39minputs[\u001b[39m1\u001b[39m],\n\u001b[0;32m     58\u001b[0m         dilations\u001b[39m=\u001b[39mop\u001b[39m.\u001b[39mget_attr(\u001b[39m\"\u001b[39m\u001b[39mdilations\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     59\u001b[0m         strides\u001b[39m=\u001b[39mop\u001b[39m.\u001b[39mget_attr(\u001b[39m\"\u001b[39m\u001b[39mstrides\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     60\u001b[0m         padding\u001b[39m=\u001b[39mop\u001b[39m.\u001b[39mget_attr(\u001b[39m\"\u001b[39m\u001b[39mpadding\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     61\u001b[0m         explicit_paddings\u001b[39m=\u001b[39mop\u001b[39m.\u001b[39mget_attr(\u001b[39m\"\u001b[39m\u001b[39mexplicit_paddings\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     62\u001b[0m         use_cudnn_on_gpu\u001b[39m=\u001b[39mop\u001b[39m.\u001b[39mget_attr(\u001b[39m\"\u001b[39m\u001b[39muse_cudnn_on_gpu\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     63\u001b[0m         data_format\u001b[39m=\u001b[39mop\u001b[39m.\u001b[39mget_attr(\u001b[39m\"\u001b[39m\u001b[39mdata_format\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mdecode())\n\u001b[0;32m     64\u001b[0m ]\n",
      "File \u001b[1;32mf:\\Code\\python\\AILearn\\new_env\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py:1255\u001b[0m, in \u001b[0;36mconv2d_backprop_filter\u001b[1;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   1254\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1255\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   1256\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mConv2DBackpropFilter\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, \u001b[39minput\u001b[39;49m, filter_sizes, out_backprop,\n\u001b[0;32m   1257\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mstrides\u001b[39;49m\u001b[39m\"\u001b[39;49m, strides, \u001b[39m\"\u001b[39;49m\u001b[39muse_cudnn_on_gpu\u001b[39;49m\u001b[39m\"\u001b[39;49m, use_cudnn_on_gpu, \u001b[39m\"\u001b[39;49m\u001b[39mpadding\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1258\u001b[0m       padding, \u001b[39m\"\u001b[39;49m\u001b[39mexplicit_paddings\u001b[39;49m\u001b[39m\"\u001b[39;49m, explicit_paddings, \u001b[39m\"\u001b[39;49m\u001b[39mdata_format\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1259\u001b[0m       data_format, \u001b[39m\"\u001b[39;49m\u001b[39mdilations\u001b[39;49m\u001b[39m\"\u001b[39;49m, dilations)\n\u001b[0;32m   1260\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   1261\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(22)\n",
    "np.random.seed(22)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "assert tf.__version__.startswith('2.')\n",
    "\n",
    "\n",
    "# hyper parameters\n",
    "z_dim = 100\n",
    "epochs = 3000000\n",
    "batch_size = 512\n",
    "learning_rate = 0.002\n",
    "is_training = True\n",
    "\n",
    "# 获取所有符合条件的路径\n",
    "img_path = glob.glob(r'faces\\*.jpg')\n",
    "\n",
    "dataset, img_shape, _ = make_anime_dataset(img_path, batch_size)\n",
    "print(dataset, img_shape)\n",
    "sample = next(iter(dataset))\n",
    "print(sample.shape, tf.reduce_max(sample).numpy(),\n",
    "        tf.reduce_min(sample).numpy())\n",
    "dataset = dataset.repeat()\n",
    "db_iter = iter(dataset)\n",
    "\n",
    "\n",
    "generator = Generator()\n",
    "generator.build(input_shape = (None, z_dim))\n",
    "discriminator = Discriminator()\n",
    "discriminator.build(input_shape=(None, 64, 64, 3))\n",
    "\n",
    "g_optimizer = tf.optimizers.Adam(learning_rate=learning_rate, beta_1=0.5)\n",
    "d_optimizer = tf.optimizers.Adam(learning_rate=learning_rate, beta_1=0.5)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    batch_z = tf.random.uniform([batch_size, z_dim], minval=-1., maxval=1.)\n",
    "    batch_x = next(db_iter)\n",
    "\n",
    "    # train D\n",
    "    with tf.GradientTape() as tape:\n",
    "        d_loss = d_loss_fn(generator, discriminator, batch_z, batch_x, is_training)\n",
    "    grads = tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "    d_optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n",
    "\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        g_loss = g_loss_fn(generator, discriminator, batch_z, is_training)\n",
    "    grads = tape.gradient(g_loss, generator.trainable_variables)\n",
    "    g_optimizer.apply_gradients(zip(grads, generator.trainable_variables))\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, 'd-loss:',float(d_loss), 'g-loss:', float(g_loss))\n",
    "\n",
    "        z = tf.random.uniform([100, z_dim])\n",
    "        fake_image = generator(z, training=False)\n",
    "        img_path = os.path.join('images', 'gan-%d.png'%epoch)\n",
    "        save_result(fake_image.numpy(), 10, img_path, color_mode='P')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('new_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d58ff1fa528449bec86ac3b87e5b9edec9a0f46f3f9c706338d5512923abfa3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
