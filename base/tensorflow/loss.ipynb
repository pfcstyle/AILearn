{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss\n",
    "\n",
    "### MSE 均方差\n",
    "\n",
    "非凸函数，可能是局部收敛\n",
    "\n",
    "$ loss = \\frac{1}{N}\\sum(y - out)^2 $\n",
    "\n",
    "$ L_{2-norm} = \\sqrt{\\sum(y - out)^2} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.constant([1, 2, 3, 0, 2])\n",
    "tf.one_hot(y, depth = 4)\n",
    "tf.cast(y, dtype = tf.float32)\n",
    "out = tf.random.normal[5, 4]\n",
    "\n",
    "loss1 = tf.reduce_mean(tf.square(y - out))\n",
    "\n",
    "loss2 = tf.square(tf.norm(y - out)) / ( 5 * 4)\n",
    "\n",
    "loss3 = tf.reduce_mean(tf.losses.MSE(y, out)) # tf.losses.MeanSquaredError是类，与MSE不同\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy 交叉熵\n",
    "信息熵是不确定性的度量，越大，不确定性越大。有效信息的输入，可以减小事情的不确定性，从而熵会降低（熵减）。\n",
    "\n",
    "交叉熵越小，模型拟合越好。\n",
    "\n",
    "这里log以2为底,P(i)是真实概率， Q（i)是预测概率\n",
    "\n",
    "$ Entropy = - \\sum_i(P(i)logQ(i)) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.3862944>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.fill([4], 0.25)\n",
    "# tensorflow里log默认以e为底\n",
    "a * tf.math.log(a) / tf.math.log(2.)\n",
    "-tf.reduce_sum(a * tf.math.log(a) / tf.math.log(2.))\n",
    "a = tf.constant([0.1, 0.1, 0.1, 0.7])\n",
    "-tf.reduce_sum(a * tf.math.log(a) / tf.math.log(2.))\n",
    "a = tf.constant([0.01, 0.01, 0.01, 0.97])\n",
    "-tf.reduce_sum(a * tf.math.log(a) / tf.math.log(2.))\n",
    "\n",
    "# 多分类  第一个参数必须做one_hot encoding  一般来说，from_logits必须要设置，否则很容易出现数值不稳定。 \n",
    "tf.losses.categorical_crossentropy([0, 1, 0, 0], [0.25, 0.25, 0.25, 0.25], from_logits=True) # tf.losses.CategoricalCrossentropy 类\n",
    "tf.losses.CategoricalCrossentropy()([0, 1, 0, 0], [0.25, 0.25, 0.25, 0.25])\n",
    "# 二分类交叉熵，只输出一个值，另一种可能就是1-result\n",
    "tf.losses.BinaryCrossentropy()([1], [0.1])\n",
    "tf.losses.binary_crossentropy([1], [0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为什么不使用MSE\n",
    "容易出现gradient vanish(梯度消失)\n",
    "\n",
    "交叉熵梯度递减更快\n",
    "\n",
    "但是在某些领域，比如meta-learning，发现MSE更科学稳定，交叉熵不稳定\n",
    "\n",
    "需要多长时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('new_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d58ff1fa528449bec86ac3b87e5b9edec9a0f46f3f9c706338d5512923abfa3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
