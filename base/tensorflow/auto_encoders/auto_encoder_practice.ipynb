{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  os\n",
    "import  tensorflow as tf\n",
    "import  numpy as np\n",
    "from    tensorflow import keras\n",
    "from    keras import Sequential, layers\n",
    "from    PIL import Image\n",
    "from    matplotlib import pyplot as plt\n",
    "\n",
    "tf.random.set_seed(22)\n",
    "np.random.seed(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(imgs, name):\n",
    "    new_im = Image.new('L', (280, 280))\n",
    "\n",
    "    index = 0\n",
    "    for i in range(0, 280, 28):\n",
    "        for j in range(0, 280, 28):\n",
    "            im = imgs[index]\n",
    "            im = Image.fromarray(im, mode='L')\n",
    "            new_im.paste(im, (i, j))\n",
    "            index += 1\n",
    "\n",
    "    new_im.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "h_dim = 20\n",
    "batchsz = 512\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "x_train, x_test = x_train.astype(np.float32) / 255., x_test.astype(np.float32) / 255.\n",
    "# we do not need label\n",
    "train_db = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_db = train_db.shuffle(batchsz * 5).batch(batchsz)\n",
    "test_db = tf.data.Dataset.from_tensor_slices(x_test)\n",
    "test_db = test_db.batch(batchsz)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AE, self).__init__()\n",
    "\n",
    "        # Encoders\n",
    "        self.encoder = Sequential([\n",
    "            layers.Dense(256, activation=tf.nn.relu),\n",
    "            layers.Dense(128, activation=tf.nn.relu),\n",
    "            layers.Dense(h_dim)\n",
    "        ])\n",
    "\n",
    "        # Decoders\n",
    "        self.decoder = Sequential([\n",
    "            layers.Dense(128, activation=tf.nn.relu),\n",
    "            layers.Dense(256, activation=tf.nn.relu),\n",
    "            layers.Dense(784)\n",
    "        ])\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # [b, 784] => [b, 10]\n",
    "        h = self.encoder(inputs)\n",
    "        # [b, 10] => [b, 784]\n",
    "        x_hat = self.decoder(h)\n",
    "\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ae_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_4 (Sequential)   (None, 20)                236436    \n",
      "                                                                 \n",
      " sequential_5 (Sequential)   (None, 784)               237200    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 473,636\n",
      "Trainable params: 473,636\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "0 0 0.6934701800346375\n",
      "0 100 0.32967621088027954\n",
      "1 0 0.32188236713409424\n",
      "1 100 0.31333833932876587\n",
      "2 0 0.3013639748096466\n",
      "2 100 0.3136025071144104\n",
      "3 0 0.2989884614944458\n",
      "3 100 0.29724690318107605\n",
      "4 0 0.2949564754962921\n",
      "4 100 0.2822934687137604\n",
      "5 0 0.27928534150123596\n",
      "5 100 0.2868483364582062\n",
      "6 0 0.28446972370147705\n",
      "6 100 0.2880308926105499\n",
      "7 0 0.2897784113883972\n",
      "7 100 0.28706738352775574\n",
      "8 0 0.2766054570674896\n",
      "8 100 0.2837865948677063\n",
      "9 0 0.2850651741027832\n",
      "9 100 0.289631187915802\n",
      "10 0 0.2736796736717224\n",
      "10 100 0.2848539352416992\n",
      "11 0 0.27832695841789246\n",
      "11 100 0.27505138516426086\n",
      "12 0 0.27563315629959106\n",
      "12 100 0.28155362606048584\n",
      "13 0 0.269102543592453\n",
      "13 100 0.2816183567047119\n",
      "14 0 0.2786197364330292\n",
      "14 100 0.2824264466762543\n",
      "15 0 0.27335256338119507\n",
      "15 100 0.27735811471939087\n",
      "16 0 0.27067697048187256\n",
      "16 100 0.2772064208984375\n",
      "17 0 0.2791179120540619\n",
      "17 100 0.2820364236831665\n",
      "18 0 0.2775062322616577\n",
      "18 100 0.2737146019935608\n",
      "19 0 0.27072054147720337\n",
      "19 100 0.284488320350647\n",
      "20 0 0.27250924706459045\n",
      "20 100 0.2777370810508728\n",
      "21 0 0.26661866903305054\n",
      "21 100 0.2772267758846283\n",
      "22 0 0.2659347951412201\n",
      "22 100 0.27709874510765076\n",
      "23 0 0.2710997462272644\n",
      "23 100 0.27355077862739563\n",
      "24 0 0.26947221159935\n",
      "24 100 0.27907058596611023\n",
      "25 0 0.27008959650993347\n",
      "25 100 0.2736572027206421\n",
      "26 0 0.270857036113739\n",
      "26 100 0.27686795592308044\n",
      "27 0 0.2731908857822418\n",
      "27 100 0.2731245160102844\n",
      "28 0 0.2677428126335144\n",
      "28 100 0.2750909924507141\n",
      "29 0 0.2780439555644989\n",
      "29 100 0.2757015824317932\n",
      "30 0 0.26793503761291504\n",
      "30 100 0.263611376285553\n",
      "31 0 0.27232789993286133\n",
      "31 100 0.2785213589668274\n",
      "32 0 0.27050143480300903\n",
      "32 100 0.26941537857055664\n",
      "33 0 0.25999248027801514\n",
      "33 100 0.27109307050704956\n",
      "34 0 0.2664751410484314\n",
      "34 100 0.2696596384048462\n",
      "35 0 0.2698831558227539\n",
      "35 100 0.2722153663635254\n",
      "36 0 0.26253753900527954\n",
      "36 100 0.2728073298931122\n",
      "37 0 0.27106136083602905\n",
      "37 100 0.2671498656272888\n",
      "38 0 0.2684881091117859\n",
      "38 100 0.2762748599052429\n",
      "39 0 0.26645541191101074\n",
      "39 100 0.26663652062416077\n",
      "40 0 0.26777926087379456\n",
      "40 100 0.26776087284088135\n",
      "41 0 0.2705458402633667\n",
      "41 100 0.27605727314949036\n",
      "42 0 0.26653146743774414\n",
      "42 100 0.277146577835083\n",
      "43 0 0.2725776433944702\n",
      "43 100 0.26946231722831726\n",
      "44 0 0.25930356979370117\n",
      "44 100 0.2793709635734558\n",
      "45 0 0.2683921456336975\n",
      "45 100 0.27397823333740234\n",
      "46 0 0.26715755462646484\n",
      "46 100 0.2709767818450928\n",
      "47 0 0.27230602502822876\n",
      "47 100 0.27120187878608704\n",
      "48 0 0.26653191447257996\n",
      "48 100 0.2803901731967926\n",
      "49 0 0.2676650583744049\n",
      "49 100 0.27556559443473816\n",
      "50 0 0.2616002857685089\n",
      "50 100 0.27310121059417725\n",
      "51 0 0.2677021622657776\n",
      "51 100 0.2728646397590637\n",
      "52 0 0.2720046043395996\n",
      "52 100 0.27075743675231934\n",
      "53 0 0.2653755247592926\n",
      "53 100 0.270495742559433\n",
      "54 0 0.27093344926834106\n",
      "54 100 0.26885488629341125\n",
      "55 0 0.26460984349250793\n",
      "55 100 0.2741307020187378\n",
      "56 0 0.2688615918159485\n",
      "56 100 0.2694613039493561\n",
      "57 0 0.26958978176116943\n",
      "57 100 0.26971718668937683\n",
      "58 0 0.269217848777771\n",
      "58 100 0.2737380266189575\n",
      "59 0 0.27135467529296875\n",
      "59 100 0.27297839522361755\n",
      "60 0 0.26856619119644165\n",
      "60 100 0.266706645488739\n",
      "61 0 0.2676531970500946\n",
      "61 100 0.26928406953811646\n",
      "62 0 0.2696680426597595\n",
      "62 100 0.2723565697669983\n",
      "63 0 0.26608800888061523\n",
      "63 100 0.26610150933265686\n",
      "64 0 0.2678159773349762\n",
      "64 100 0.2648710608482361\n",
      "65 0 0.26816993951797485\n",
      "65 100 0.2683316767215729\n",
      "66 0 0.26084667444229126\n",
      "66 100 0.2752111256122589\n",
      "67 0 0.26207098364830017\n",
      "67 100 0.2679309546947479\n",
      "68 0 0.27167701721191406\n",
      "68 100 0.26760023832321167\n",
      "69 0 0.2669447064399719\n",
      "69 100 0.275729775428772\n",
      "70 0 0.26611071825027466\n",
      "70 100 0.2667232155799866\n",
      "71 0 0.2623331546783447\n",
      "71 100 0.27111315727233887\n",
      "72 0 0.2710244655609131\n",
      "72 100 0.267199844121933\n",
      "73 0 0.2653452455997467\n",
      "73 100 0.2763024568557739\n",
      "74 0 0.25892388820648193\n",
      "74 100 0.27506646513938904\n",
      "75 0 0.2738042175769806\n",
      "75 100 0.2743692100048065\n",
      "76 0 0.2647911608219147\n",
      "76 100 0.26233839988708496\n",
      "77 0 0.26760411262512207\n",
      "77 100 0.27120736241340637\n",
      "78 0 0.27285999059677124\n",
      "78 100 0.276006817817688\n",
      "79 0 0.26498132944107056\n",
      "79 100 0.27597367763519287\n",
      "80 0 0.26822176575660706\n",
      "80 100 0.2657027244567871\n",
      "81 0 0.26067760586738586\n",
      "81 100 0.2660735249519348\n",
      "82 0 0.26930713653564453\n",
      "82 100 0.2727564871311188\n",
      "83 0 0.26885512471199036\n",
      "83 100 0.2686668038368225\n",
      "84 0 0.2615184187889099\n",
      "84 100 0.26525020599365234\n",
      "85 0 0.26929986476898193\n",
      "85 100 0.27479439973831177\n",
      "86 0 0.2648043632507324\n",
      "86 100 0.272774338722229\n",
      "87 0 0.26866215467453003\n",
      "87 100 0.27534472942352295\n",
      "88 0 0.2624214291572571\n",
      "88 100 0.27444928884506226\n",
      "89 0 0.26748794317245483\n",
      "89 100 0.26443153619766235\n",
      "90 0 0.26454997062683105\n",
      "90 100 0.2630140781402588\n",
      "91 0 0.26477378606796265\n",
      "91 100 0.2687261700630188\n",
      "92 0 0.272411972284317\n",
      "92 100 0.26523149013519287\n",
      "93 0 0.2692715525627136\n",
      "93 100 0.266412615776062\n",
      "94 0 0.2758922278881073\n",
      "94 100 0.26784154772758484\n",
      "95 0 0.26273807883262634\n",
      "95 100 0.2667015790939331\n",
      "96 0 0.263630211353302\n",
      "96 100 0.2643985450267792\n",
      "97 0 0.26540976762771606\n",
      "97 100 0.2717554569244385\n",
      "98 0 0.2707480192184448\n",
      "98 100 0.26808834075927734\n",
      "99 0 0.2619645893573761\n",
      "99 100 0.26758646965026855\n"
     ]
    }
   ],
   "source": [
    "model = AE()\n",
    "model.build(input_shape=(None, 784))\n",
    "model.summary()\n",
    "\n",
    "optimizer = tf.optimizers.Adam(lr=lr)\n",
    "\n",
    "for epoch in range(100):\n",
    "\n",
    "    for step, x in enumerate(train_db):\n",
    "\n",
    "        #[b, 28, 28] => [b, 784]\n",
    "        x = tf.reshape(x, [-1, 784])\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            x_rec_logits = model(x)\n",
    "\n",
    "            rec_loss = tf.losses.binary_crossentropy(x, x_rec_logits, from_logits=True)\n",
    "            rec_loss = tf.reduce_mean(rec_loss)\n",
    "\n",
    "        grads = tape.gradient(rec_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "\n",
    "        if step % 100 ==0:\n",
    "            print(epoch, step, float(rec_loss))\n",
    "\n",
    "\n",
    "        # evaluation\n",
    "        x = next(iter(test_db))\n",
    "        logits = model(tf.reshape(x, [-1, 784]))\n",
    "        x_hat = tf.sigmoid(logits)\n",
    "        # [b, 784] => [b, 28, 28]\n",
    "        x_hat = tf.reshape(x_hat, [-1, 28, 28])\n",
    "\n",
    "        # [b, 28, 28] => [2b, 28, 28]\n",
    "        x_concat = tf.concat([x, x_hat], axis=0)\n",
    "        x_concat = x_hat\n",
    "        x_concat = x_concat.numpy() * 255.\n",
    "        x_concat = x_concat.astype(np.uint8)\n",
    "        save_images(x_concat, 'save_images/rec_epoch_%d.png'%epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('new_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d58ff1fa528449bec86ac3b87e5b9edec9a0f46f3f9c706338d5512923abfa3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
