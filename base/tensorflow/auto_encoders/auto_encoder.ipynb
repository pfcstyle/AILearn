{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Encoder\n",
    "本质上就是一个简单的feature forward神经网络，属于无监督学习，以自己为目标进行训练。\n",
    "\n",
    "![auto_encoder](../images/auto_encoder.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to train(Loss)\n",
    "\n",
    "![auto_encoder_loss](../images/auto_encoder_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising(去噪) AutoEncoders\n",
    "核心思想是为了避免训练时，直接记住了像素点，给数据增加一些噪声，如果还能训练出来，说明真的捕捉到了关键高维特征。\n",
    "\n",
    "![denoising_auto_encoders](../images/denoising_auto_encoders.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout AutoEncoders\n",
    "\n",
    "就是有选择（一般随机）的断掉一些weight的计算，可以提高收敛速度，降低计算量。参数需要不断调整。\n",
    "\n",
    "![dropout_auto_encoders](../images/dropout_auto_encoders.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial AutoEncoders\n",
    "\n",
    "核心思想是，希望encoder得到的特征可以符合某种分布（比如正太分布），那么就构造一个期望的分布，来与实际encode的特征进行对比判断，从而给予反馈，让真正encode的特征逐步符合期望的分布。\n",
    "\n",
    "![adversarial_auto_encoders](../images/adversarial_auto_encoders.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Auto Encoder\n",
    "\n",
    "自动编码器不能任意生成图片，因为我们没有办法自己去构造隐藏向量，我们需要通过一张图片输入编码我们才知道得到的隐含向量是什么，这时我们就可以通过变分自动编码器来解决这个问题。原理特别简单，只需要在编码过程给它增加一些限制，迫使其生成的隐含向量能够粗略的遵循一个标准正态分布，这就是其与一般的自动编码器最大的不同。\n",
    "\n",
    "在实际情况中，我们需要在模型的准确率上与隐含向量服从标准正态分布之间做一个权衡，所谓模型的准确率就是指解码器生成的图片与原图片的相似程度。我们可以让网络自己来做这个决定，非常简单，我们只需要将这两者都做一个loss，然后在将他们求和作为总的loss，这样网络就能够自己选择如何才能够使得这个总的loss下降。\n",
    "\n",
    "![](../images/variational_auto_encoder_network.png)\n",
    "\n",
    "下图中$\\theta$是编码过程， $\\phi$是解码过程。$q_\\theta$是x的分布，$logp_{\\phi}(x_i|z)$是给定z，得到$x_i$的概率，使用log单调性不变，但是增减了非线性能力。\n",
    "\n",
    "![variational_auto_encoder](../images/variational_auto_encoder.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KL divergence\n",
    "\n",
    "KL divergence用来衡量两种分布的相似程度，这里我们就是用KL divergence来表示隐含向量与标准正态分布之间差异的loss。\n",
    "\n",
    "$DKL(P||Q) = \\int_{-\\infty}^{\\infty}p(x)log\\frac{p(x)}{q(x)}dx $\n",
    "\n",
    "p(x)和q(x)分布重叠越大，得到的面积越小，KL值越小。\n",
    "\n",
    "![kl_demo](../images/kl_demo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reparameterization trick\n",
    "\n",
    "我们发现，x到z的过程，先训练出正太分布参数，然后从分布中再采样得到z。但是sample过程是不可导的，意味着无法求导，无法梯度训练。通过reparameterization trick解决这个问题。\n",
    "\n",
    "其实思想很简单，从从$N(\\mu, \\sigma^2)$中采样一个Z, 相当于从$N(0, 1)$中采样一个$\\epsilon$, 然后让$Z = \\mu + \\epsilon \\times \\sigma$\n",
    "\n",
    "![](../images/variational_auto_encoder_sample.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('new_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d58ff1fa528449bec86ac3b87e5b9edec9a0f46f3f9c706338d5512923abfa3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
