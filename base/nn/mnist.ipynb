{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 4s 0us/step\n",
      "datasets: (60000, 28, 28) (60000,) 0 255\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n",
    "\n",
    "# 下载minist数据\n",
    "(xs, ys),_ = datasets.mnist.load_data()\n",
    "print('datasets:', xs.shape, ys.shape, xs.min(), xs.max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理数据\n",
    "\n",
    "xs = tf.convert_to_tensor(xs, dtype=tf.float32) / 255.\n",
    "db = tf.data.Dataset.from_tensor_slices((xs,ys))\n",
    "db = db.batch(32).repeat(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               200960    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 335,114\n",
      "Trainable params: 335,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 构建网络\n",
    "network = Sequential([layers.Dense(256, activation='relu'),\n",
    "                     layers.Dense(256, activation='relu'),\n",
    "                     layers.Dense(256, activation='relu'),\n",
    "                     layers.Dense(10)])\n",
    "network.build(input_shape=(None, 28*28))\n",
    "network.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Code\\python\\AILearn\\env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 1.5347081422805786 acc: 0.15625\n",
      "200 loss: 0.42675477266311646 acc: 0.6875\n",
      "400 loss: 0.3711259067058563 acc: 0.8421875\n",
      "600 loss: 0.32421213388442993 acc: 0.865\n",
      "800 loss: 0.261374294757843 acc: 0.8957813\n",
      "1000 loss: 0.2811684012413025 acc: 0.8990625\n",
      "1200 loss: 0.28251758217811584 acc: 0.91203123\n",
      "1400 loss: 0.22117739915847778 acc: 0.9139063\n",
      "1600 loss: 0.21573254466056824 acc: 0.91125\n",
      "1800 loss: 0.1878446489572525 acc: 0.9298437\n",
      "2000 loss: 0.23168635368347168 acc: 0.9409375\n",
      "2200 loss: 0.13439443707466125 acc: 0.93078125\n",
      "2400 loss: 0.21195188164710999 acc: 0.92859375\n",
      "2600 loss: 0.19127200543880463 acc: 0.9375\n",
      "2800 loss: 0.13681891560554504 acc: 0.938125\n",
      "3000 loss: 0.2136998474597931 acc: 0.936875\n",
      "3200 loss: 0.17318466305732727 acc: 0.936875\n",
      "3400 loss: 0.1528593897819519 acc: 0.93625\n",
      "3600 loss: 0.125424325466156 acc: 0.93875\n",
      "3800 loss: 0.15749666094779968 acc: 0.9560937\n",
      "4000 loss: 0.20749050378799438 acc: 0.9529688\n",
      "4200 loss: 0.12807586789131165 acc: 0.9428125\n",
      "4400 loss: 0.16294194757938385 acc: 0.9482812\n",
      "4600 loss: 0.18821263313293457 acc: 0.946875\n",
      "4800 loss: 0.13436385989189148 acc: 0.94703126\n",
      "5000 loss: 0.12408225238323212 acc: 0.950625\n",
      "5200 loss: 0.1994556039571762 acc: 0.944375\n",
      "5400 loss: 0.2036987841129303 acc: 0.9484375\n",
      "5600 loss: 0.08885611593723297 acc: 0.9610937\n",
      "5800 loss: 0.14854413270950317 acc: 0.95984375\n",
      "6000 loss: 0.1184958890080452 acc: 0.955625\n",
      "6200 loss: 0.1496010422706604 acc: 0.9529688\n",
      "6400 loss: 0.10262863337993622 acc: 0.9557812\n",
      "6600 loss: 0.10757151246070862 acc: 0.95484376\n",
      "6800 loss: 0.09230322390794754 acc: 0.9565625\n",
      "7000 loss: 0.10284538567066193 acc: 0.95625\n",
      "7200 loss: 0.26137661933898926 acc: 0.9496875\n",
      "7400 loss: 0.1324133574962616 acc: 0.95875\n",
      "7600 loss: 0.16110607981681824 acc: 0.966875\n",
      "7800 loss: 0.08001449704170227 acc: 0.95953125\n",
      "8000 loss: 0.16084441542625427 acc: 0.9596875\n",
      "8200 loss: 0.059957247227430344 acc: 0.961875\n",
      "8400 loss: 0.06512018293142319 acc: 0.9579688\n",
      "8600 loss: 0.09672465920448303 acc: 0.96\n",
      "8800 loss: 0.11246389150619507 acc: 0.96171874\n",
      "9000 loss: 0.1270432472229004 acc: 0.9557812\n",
      "9200 loss: 0.06111469119787216 acc: 0.9607813\n",
      "9400 loss: 0.07066649943590164 acc: 0.9710938\n",
      "9600 loss: 0.16311080753803253 acc: 0.9653125\n",
      "9800 loss: 0.047288987785577774 acc: 0.96203125\n",
      "10000 loss: 0.16272777318954468 acc: 0.9639062\n",
      "10200 loss: 0.10745466500520706 acc: 0.9615625\n",
      "10400 loss: 0.1296308934688568 acc: 0.9615625\n",
      "10600 loss: 0.07843128591775894 acc: 0.9665625\n",
      "10800 loss: 0.16927240788936615 acc: 0.9610937\n",
      "11000 loss: 0.08371840417385101 acc: 0.9607813\n",
      "11200 loss: 0.10626933723688126 acc: 0.96953124\n",
      "11400 loss: 0.09573909640312195 acc: 0.97078127\n",
      "11600 loss: 0.14577947556972504 acc: 0.96515626\n",
      "11800 loss: 0.07832426577806473 acc: 0.9665625\n",
      "12000 loss: 0.07258159667253494 acc: 0.966875\n",
      "12200 loss: 0.06595347076654434 acc: 0.9660938\n",
      "12400 loss: 0.10537971556186676 acc: 0.96765625\n",
      "12600 loss: 0.15282213687896729 acc: 0.965\n",
      "12800 loss: 0.11946847289800644 acc: 0.9639062\n",
      "13000 loss: 0.09043926000595093 acc: 0.9664062\n",
      "13200 loss: 0.1515137255191803 acc: 0.9739063\n",
      "13400 loss: 0.08105966448783875 acc: 0.971875\n",
      "13600 loss: 0.08381825685501099 acc: 0.9671875\n",
      "13800 loss: 0.09554462134838104 acc: 0.97125\n",
      "14000 loss: 0.05394226312637329 acc: 0.9685938\n",
      "14200 loss: 0.17883580923080444 acc: 0.96953124\n",
      "14400 loss: 0.09089953452348709 acc: 0.969375\n",
      "14600 loss: 0.14510796964168549 acc: 0.9660938\n",
      "14800 loss: 0.08852703124284744 acc: 0.9646875\n",
      "15000 loss: 0.07845781743526459 acc: 0.9765625\n",
      "15200 loss: 0.10190583765506744 acc: 0.9721875\n",
      "15400 loss: 0.09243965148925781 acc: 0.970625\n",
      "15600 loss: 0.07031694054603577 acc: 0.97296876\n",
      "15800 loss: 0.08770069479942322 acc: 0.9715625\n",
      "16000 loss: 0.10717208683490753 acc: 0.96734375\n",
      "16200 loss: 0.08101575076580048 acc: 0.9714062\n",
      "16400 loss: 0.08412919193506241 acc: 0.97125\n",
      "16600 loss: 0.0751941129565239 acc: 0.96453124\n",
      "16800 loss: 0.08517380803823471 acc: 0.9735938\n",
      "17000 loss: 0.09239291399717331 acc: 0.97703123\n",
      "17200 loss: 0.03204482048749924 acc: 0.97234374\n",
      "17400 loss: 0.07411032915115356 acc: 0.9725\n",
      "17600 loss: 0.0781838595867157 acc: 0.97375\n",
      "17800 loss: 0.05905938893556595 acc: 0.97265625\n",
      "18000 loss: 0.06814288347959518 acc: 0.97296876\n",
      "18200 loss: 0.08924780040979385 acc: 0.9714062\n",
      "18400 loss: 0.06056588888168335 acc: 0.97078127\n",
      "18600 loss: 0.052990105003118515 acc: 0.96953124\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "optimizer = optimizers.SGD(lr=0.01)\n",
    "acc_meter = metrics.Accuracy()\n",
    "\n",
    "for step, (x,y) in enumerate(db):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # [b, 28, 28] => [b, 784]\n",
    "        x = tf.reshape(x, (-1, 28*28))\n",
    "        # [b, 784] => [b, 10]\n",
    "        out = network(x)\n",
    "        # [b] => [b, 10]\n",
    "        y_onehot = tf.one_hot(y, depth=10)\n",
    "        # [b, 10]\n",
    "        loss = tf.square(out-y_onehot)\n",
    "        # [b]\n",
    "        loss = tf.reduce_sum(loss) / 32\n",
    "\n",
    "\n",
    "    acc_meter.update_state(tf.argmax(out, axis=1), y)\n",
    "\n",
    "    grads = tape.gradient(loss, network.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
    "\n",
    "\n",
    "    if step % 200==0:\n",
    "\n",
    "        print(step, 'loss:', float(loss), 'acc:', acc_meter.result().numpy())\n",
    "        acc_meter.reset_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bafc338abc57834bacf3c306014ce0c45f0aeaf201b721c6f2bfe17ccf7009fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
